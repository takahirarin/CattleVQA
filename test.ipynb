{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "#from transformer_block import MultiHeadAttention, FFN, SubLayerConnection\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class ViltModel(nn.Module):\n",
    "    def __init__(self, config: ViltConfig):\n",
    "        \"\"\"\"config: 'ViltConfig' instance \"\"\"\n",
    "        super(ViltModel, self).__init__()\n",
    "        self.embeddings = ViltEmbeddings(config)\n",
    "        self.encoder = ViltEncoder(config)\n",
    "        self.pooler = ViltPooler(config)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.classifier = ViltClassifer(config)\n",
    "\n",
    "\n",
    "    def get_extended_attention_mask(self, attention_mask, input_shape, device):\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            # if config.is_decoder:\n",
    "            #     batch_size, seq_length = input_shape\n",
    "            #     seq_ids = torch.arange(seq_length, device=device)\n",
    "            #     causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "            #     # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "            #     # causal and attention masks must have same type with pytorch version < 1.3\n",
    "            #     causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "            #     if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "            #         prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "            #         causal_mask = torch.cat(\n",
    "            #             [\n",
    "            #                 torch.ones(\n",
    "            #                     (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
    "            #                 ),\n",
    "            #                 causal_mask,\n",
    "            #             ],\n",
    "            #             axis=-1,\n",
    "            #         )\n",
    "\n",
    "            #     extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            # else:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        #extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask, image_token_type_idx=1):\n",
    "        input_shape = input_ids.size()\n",
    "        print(f'input_shape: {input_shape}')\n",
    "        # get text info\n",
    "        text_batch_size, seq_length = input_shape\n",
    "        device = input_ids.device\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((text_batch_size, seq_length)), device=device)\n",
    "        # get image info\n",
    "        image_batch_size =  pixel_values.shape[0] \n",
    "        if pixel_values is None:\n",
    "            pixel_values = torch.ones((image_batch_size, self.config.image_size, self.config_image_size), device=device)\n",
    "        \n",
    "        # calculate embeddings\n",
    "        embeddings, masks = self.embeddings(\n",
    "            input_ids, attention_mask, token_type_ids,\n",
    "            pixel_values, pixel_mask,image_token_type_idx )\n",
    "\n",
    "        print(f'enbeddings: { embeddings.size()}')\n",
    "        \n",
    "        # input embeddings into encoder\n",
    "        # extended_attention_mask = ModuleUtilsMixin.get_extended_attention_mask(attention_mask, input_shape)\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "        encoder_output = self.encoder(embeddings, extended_attention_mask)\n",
    "        sequence_output = encoder_output[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        #classifier\n",
    "        output = self.classifier(pooled_output)\n",
    "        return encoder_output, pooled_output, output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ViltClassifer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(config.hidden_size, config.hidden_size*2)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size*2)\n",
    "        self.activation = config.hidden_act\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.fc(x)\n",
    "        output = self.norm(output)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self,config) :\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        layer = ViltLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers\n",
    "\n",
    "\n",
    "\n",
    "class ViltLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.transformer_block = FFN(config)\n",
    "        self.shortcut = SubLayerConnection(config)\n",
    "    def forward(self,hidden_states,attention_mask):\n",
    "        attention_output = self.attention(hidden_states, hidden_states, hidden_states,\n",
    "            config.num_attention_heads, config.hidden_size, attention_mask)\n",
    "        FFN_output = self.transformer_block(attention_output)\n",
    "        layer_output = self.shortcut(FFN_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "class ViltPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"taking the hidden state corresponding to the first token.\"\"\"\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class ViltEmbeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        #super(ViltEmbeddings).__init__()\n",
    "        super().__init__()\n",
    "         # text embeddings\n",
    "        self.text_embeddings = BERTEmbeddings(config)\n",
    "        # patch embeddings\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        #image_size = 384, patch_size = 32\n",
    "        #元のコード\n",
    "        #num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        #どちらも一次元のただの数値なので[1]とかない→多分n*nの解像度（画像サイズ）表示じゃなくて相対解像度??\n",
    "        #とりあえずimage_size // patch_sizeにした\n",
    "        \n",
    "        # num_patches = config.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        # modality type embedding\n",
    "        self.token_type_embeddings = nn.Embedding(config.modality_type_vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.config = config\n",
    "\n",
    "    def visual_embed(self,pixel_values, pixel_mask, max_image_length=200):\n",
    "        _, _, ph, pw = self.patch_embeddings.projection.weight.shape\n",
    "        print('x: {x.size()}')\n",
    "\n",
    "        x = self.patch_embeddings(pixel_values)\n",
    "        print(f'x: {x.size()}')\n",
    "        x_mask = pixel_mask[:, None, :, :].float()\n",
    "        print(f'x_mask: {x_mask.size()}')\n",
    "        x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "        x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "        x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        patch_dim = self.config.image_size // self.config.patch_size\n",
    "        spatial_pos = self.position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "        pos_embed = torch.cat(\n",
    "            [\n",
    "                nn.functional.pad(\n",
    "                    nn.functional.interpolate(\n",
    "                        spatial_pos,\n",
    "                        size=(h, w),\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=True,\n",
    "                    ),\n",
    "                    (0, width - w, 0, height - h),\n",
    "                )\n",
    "                for h, w in zip(x_h, x_w)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "         # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "        patch_index = torch.stack(\n",
    "            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "        ).to(device=x_mask.device)\n",
    "        patch_index = patch_index[None, None, :, :, :]\n",
    "        patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "        patch_index = patch_index.flatten(1, 3)\n",
    "        x_mask = x_mask.flatten(1)\n",
    "\n",
    "        if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "            # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "            # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "            # if self.patch_size = 32, 25 * 41 = 1025\n",
    "            # if res is 384 x 640, 12 * 20 = 240\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = effective_resolution.max()\n",
    "        else:\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "        valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "        non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "        unique_rows = valid_idx[:, 0].unique()\n",
    "        valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "        non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "        pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "        select = list()\n",
    "        for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "            if p <= 0:\n",
    "                valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "                select.append(valid_row_idx[i][valid_choice])\n",
    "            else:\n",
    "                pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "                select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "        select = torch.cat(select, dim=0)\n",
    "        x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "        x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "        # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "        patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "        pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        pos_embed = torch.cat(\n",
    "            (self.position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "        )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "        return x, x_mask, (patch_index, (height, width))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask,image_token_type_idx=1):\n",
    "            # 1. text embeddings\n",
    "            text_embeds = self.text_embeddings(\n",
    "                input_ids = input_ids, token_type_ids= token_type_ids )\n",
    "\n",
    "            # 2. patch embeddings\n",
    "            \"\"\"if use clip, change code here\n",
    "            for example: \n",
    "                import clip\n",
    "                model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "                image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "                image_embeds = model.encode_image(image)\n",
    "            \"\"\"\n",
    "            image_embeds, image_masks, patch_index = self.visual_embed(\n",
    "                pixel_values , pixel_mask, max_image_length=self.config.max_image_length )\n",
    "            \n",
    "            # 3. add modality type embedding\n",
    "            # text_embeds = text_embeds + self.token_type_embeddings(\n",
    "            #     torch.zeros_like(attention_mask,dtype=torch.long, device = text_embeds.device))\n",
    "\n",
    "            image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx, dtype=torch.long, device=image_embeds.device))\n",
    "\n",
    "            # 4. concat\n",
    "            embeddings = torch.cat([text_embeds, image_embeds], dim =1)\n",
    "            masks = torch.cat([attention_mask, image_masks], dim=1)\n",
    "            return embeddings, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from vilt_config import ViltConfig\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "            super(Attention, self).__init__()\n",
    "            \n",
    "    def forward(self, query, key, value, attention_mask,attention_dropout_prob):\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(query.size(-1)) #(query * key^T) / √d_k\n",
    "        # scores.size() = [batch_size, head, 1, 1]\n",
    "        # query*keyで,[batch_size, head, 1, d_k] * [batch_size, head, 1, d_k]^T = [batch_size, head, 1, 1]\n",
    "        # (実質、[1, d_k]*[d_k,1] = [1,1]となっている)\n",
    "\n",
    "        print(scores.size())\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, 0)\n",
    "            #0のところを∞に置き換えてマスクする？\n",
    "\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        #行方向にsoftmax\n",
    "\n",
    "        if attention_dropout_prob is not None:\n",
    "            attn = attention_dropout_prob(scores)\n",
    "        \n",
    "        # attn: [batch_size, head, 1, 1], value: [batch_size, head, 1, d_k]\n",
    "        # attn*value = [batch_size, head, 1, d_k]\n",
    "        return torch.matmul(attn, value), attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        #super(MultiHeadAttention).__init__()\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(config.hidden_size, config.hidden_size) for _ in range(config.num_attention_heads)])\n",
    "        self.output_linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attention = Attention(config)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,query, key, value, head, hidden_size, attention_mask=None, attention_dropout_prob=None):\n",
    "        batch_size = query.size(0)\n",
    "        d_k = hidden_size // head\n",
    "\n",
    "        query, key, value = [l(x).view(batch_size, -1, head, d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "                            #h個のattention層に分けるために(batch_size, -1, head, d_k)の形にする\n",
    "        \n",
    "        attention_output, attn = self.attention(query, key, value, attention_mask, attention_dropout_prob)\n",
    "        \n",
    "        attention_output = attention_output.transpose(1,2).contiguous().view(batch_size, -1, head * d_k)\n",
    "        #41行目でtranspose(1,2)してたので戻すためのtranspose\n",
    "        #viewを使うときは要素順に並んでいないといけないのでそのためのcontiguous()\n",
    "        #[batch_size, 1, hidden_size] で出力\n",
    "\n",
    "        return self.output_linear(attention_output)\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(FFN,self).__init__()\n",
    "        self.dense  = nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        self.activation = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dence(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "class SubLayerConnection(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(SubLayerConnection,self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def farward(self,hidden_states,input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9fdd87e8cf40ce80c51bccf78e216b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "from transformer_block import MultiHeadAttention\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "config = ViltConfig()\n",
    "# model = ViltModel(config)\n",
    "path1 = 'Dataset/questions/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "path2 = 'Dataset/annotations/v2_mscoco_val2014_annotations.json'\n",
    "questions, annotations = read_data(path1, path2, config)\n",
    "# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "dataset = VQADataset(questions, annotations, config)\n",
    "dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "# for datas in tqdm(dataloader):\n",
    "#     data = {k: v for k, v in datas.items()}\n",
    "#     data.pop('labels')\n",
    "#     print(data.keys())\n",
    "# data\n",
    "data = next(iter(dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dandelin/vilt-b32-mlm were not used when initializing ViltForQuestionAnswering: ['mlm_score.transform.LayerNorm.weight', 'mlm_score.transform.dense.bias', 'mlm_score.transform.LayerNorm.bias', 'mlm_score.transform.dense.weight', 'mlm_score.bias', 'mlm_score.decoder.weight']\n",
      "- This IS expected if you are initializing ViltForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.1.weight', 'classifier.3.weight', 'classifier.0.weight', 'classifier.0.bias', 'classifier.3.bias', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViltForQuestionAnswering\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",\n",
    "                                                 num_labels=len(config.id2label),\n",
    "                                                 id2label=config.id2label,\n",
    "                                                 label2id=config.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122e9b48541f4b568d4b465e94a9cdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values', 'pixel_mask'])\n"
     ]
    }
   ],
   "source": [
    "for datas in tqdm(dataloader):\n",
    "    data = {k: v for k, v in datas.items()}\n",
    "    data.pop('labels')\n",
    "    print(data.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 40])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: torch.Size([32, 40])\n",
      "x: {x.size()}\n",
      "x: torch.Size([32, 768, 19, 19])\n",
      "x_mask: torch.Size([32, 1, 608, 608])\n",
      "torch.Size([32, 12, 257, 257])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (40) must match the size of tensor b (257) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m ViltModel(config)\n\u001b[0;32m----> 2\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [11], line 111\u001b[0m, in \u001b[0;36mViltModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, pixel_values, pixel_mask, image_token_type_idx)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m# input embeddings into encoder\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# extended_attention_mask = ModuleUtilsMixin.get_extended_attention_mask(attention_mask, input_shape)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape, device)\n\u001b[0;32m--> 111\u001b[0m encoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(embeddings, extended_attention_mask)\n\u001b[1;32m    112\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_output[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    113\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [11], line 148\u001b[0m, in \u001b[0;36mViltEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m all_encoder_layers \u001b[39m=\u001b[39m []\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m layer_module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer:\n\u001b[0;32m--> 148\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_module(hidden_states, attention_mask)\n\u001b[1;32m    149\u001b[0m     all_encoder_layers\u001b[39m.\u001b[39mappend(hidden_states)\n\u001b[1;32m    150\u001b[0m \u001b[39mreturn\u001b[39;00m all_encoder_layers\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [11], line 161\u001b[0m, in \u001b[0;36mViltLayer.forward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,hidden_states,attention_mask):\n\u001b[0;32m--> 161\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden_states, hidden_states, hidden_states,\n\u001b[1;32m    162\u001b[0m         config\u001b[39m.\u001b[39;49mnum_attention_heads, config\u001b[39m.\u001b[39;49mhidden_size, attention_mask)\n\u001b[1;32m    163\u001b[0m     FFN_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_block(attention_output)\n\u001b[1;32m    164\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(FFN_output, attention_output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 53\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, head, hidden_size, attention_mask, attention_dropout_prob)\u001b[0m\n\u001b[1;32m     49\u001b[0m query, key, value \u001b[39m=\u001b[39m [l(x)\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, head, d_k)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     50\u001b[0m                      \u001b[39mfor\u001b[39;00m l, x \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_layers, (query, key, value))]\n\u001b[1;32m     51\u001b[0m                     \u001b[39m#h個のattention層に分けるために(batch_size, -1, head, d_k)の形にする\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m attention_output, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(query, key, value, attention_mask, attention_dropout_prob)\n\u001b[1;32m     55\u001b[0m attention_output \u001b[39m=\u001b[39m attention_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, head \u001b[39m*\u001b[39m d_k)\n\u001b[1;32m     56\u001b[0m \u001b[39m#41行目でtranspose(1,2)してたので戻すためのtranspose\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m#viewを使うときは要素順に並んでいないといけないのでそのためのcontiguous()\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m#[batch_size, 1, hidden_size] で出力\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 19\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, attention_mask, attention_dropout_prob)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(scores\u001b[39m.\u001b[39msize())\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39;49mmasked_fill(attention_mask \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m     \u001b[39m#0のところを∞に置き換えてマスクする？\u001b[39;00m\n\u001b[1;32m     22\u001b[0m attn \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (40) must match the size of tensor b (257) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "model = ViltModel(config)\n",
    "output = model(**data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataiter = iter(dataloader)\n",
    "# data1=next(dataiter)\n",
    "# data[\"pixel_values\"].size()\n",
    "\n",
    "float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, pooled_output, output = model(**data)\n",
    "\n",
    "print(encoder_output, pooled_output, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visual_embed(pixel_values, pixel_mask, max_image_length=200):\n",
    "    patch_embeddings = PatchEmbeddings(config)\n",
    "    dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    print(pixel_values.size())\n",
    "    cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "    num_patches = patch_embeddings.num_patches\n",
    "    position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "    _, _, ph, pw = patch_embeddings.projection.weight.shape\n",
    "\n",
    "    x = patch_embeddings(pixel_values)\n",
    "    print(x.size())\n",
    "    x_mask = pixel_mask[:, None, :, :].float()\n",
    "    x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "    x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "    x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    patch_dim = config.image_size // config.patch_size\n",
    "    spatial_pos = position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "    pos_embed = torch.cat(\n",
    "        [\n",
    "            nn.functional.pad(\n",
    "                nn.functional.interpolate(\n",
    "                    spatial_pos,\n",
    "                    size=(h, w),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                ),\n",
    "                (0, width - w, 0, height - h),\n",
    "            )\n",
    "            for h, w in zip(x_h, x_w)\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "        # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "    patch_index = torch.stack(\n",
    "        torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "    ).to(device=x_mask.device)\n",
    "    patch_index = patch_index[None, None, :, :, :]\n",
    "    patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "    patch_index = patch_index.flatten(1, 3)\n",
    "    x_mask = x_mask.flatten(1)\n",
    "\n",
    "    if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "        # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "        # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "        # if self.patch_size = 32, 25 * 41 = 1025\n",
    "        # if res is 384 x 640, 12 * 20 = 240\n",
    "        effective_resolution = x_h * x_w\n",
    "        max_image_length = effective_resolution.max()\n",
    "    else:\n",
    "        effective_resolution = x_h * x_w\n",
    "        max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "    valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "    non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "    unique_rows = valid_idx[:, 0].unique()\n",
    "    valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "    non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "    valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "    non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "    pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "    select = list()\n",
    "    for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "        if p <= 0:\n",
    "            valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "            select.append(valid_row_idx[i][valid_choice])\n",
    "        else:\n",
    "            pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "            select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "    select = torch.cat(select, dim=0)\n",
    "    x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "    x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "    # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "    patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "    pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "    cls_tokens = cls_token.expand(batch_size, -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "    pos_embed = torch.cat(\n",
    "        (position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "    )\n",
    "    x = x + pos_embed\n",
    "    x = dropout(x)\n",
    "\n",
    "    x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "    return x, x_mask, (patch_index, (height, width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vilt_model import ViltEmbeddings\n",
    "s= data\n",
    "\n",
    "visual_embed(s[\"pixel_values\"],s[\"pixel_mask\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())\n",
    "# last_hidden_state = outputs.last_hidden_state\n",
    "# pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa38bdac91f012d3aa63752192202c1c4a31351885bf71923469150acc0ed662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
