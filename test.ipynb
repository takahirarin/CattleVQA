{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "from transformer_block import MultiHeadAttention\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class ViltModel(nn.Module):\n",
    "    def __init__(self, config: ViltConfig):\n",
    "        \"\"\"\"config: 'ViltConfig' instance \"\"\"\n",
    "        super(ViltModel, self).__init__()\n",
    "        self.embeddings = ViltEmbeddings(config)\n",
    "        self.encoder = ViltEncoder(config)\n",
    "        self.pooler = ViltPooler(config)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.classifier = ViltClassifer(config)\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask, image_token_type_idx=1):\n",
    "        input_shape = input_ids.size()\n",
    "        # get text info\n",
    "        text_batch_size, seq_length = input_shape\n",
    "        device = input_ids.device\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((text_batch_size, seq_length)), device=device)\n",
    "        # get image info\n",
    "        image_batch_size =  pixel_values.shape[0] \n",
    "        if pixel_values is None:\n",
    "            pixel_values = torch.ones((image_batch_size, self.config.image_size, self.config_image_size), device=device)\n",
    "        \n",
    "        # calculate embeddings\n",
    "        embeddings, masks = self.embeddings(\n",
    "            input_ids, attention_mask, token_type_ids,\n",
    "            pixel_values, pixel_mask,image_token_type_idx )\n",
    "        \n",
    "        # input embeddings into encoder\n",
    "        extended_attention_mask = ModuleUtilsMixin.get_extended_attention_mask(attention_mask, input_shape)\n",
    "        encoder_output = self.encoder(embeddings, extended_attention_mask)\n",
    "        sequence_output = encoder_output[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        #classifier\n",
    "        output = self.classifier(pooled_output)\n",
    "        return encoder_output, pooled_output, output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ViltClassifer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(config.hidden_size, config.hidden_size*2)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size*2)\n",
    "        self.activation = config.hidden_act\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.fc(x)\n",
    "        output = self.norm(output)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self,config) :\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        layer = ViltLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers\n",
    "\n",
    "\n",
    "\n",
    "class ViltLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "\n",
    "\n",
    "class ViltPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"taking the hidden state corresponding to the first token.\"\"\"\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class ViltEmbeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        #super(ViltEmbeddings).__init__()\n",
    "        super().__init__()\n",
    "         # text embeddings\n",
    "        self.text_embeddings = BERTEmbeddings(config)\n",
    "        # patch embeddings\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_patches = image_size // patch_size\n",
    "        #image_size = 384, patch_size = 32\n",
    "        #元のコード\n",
    "        #num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        #どちらも一次元のただの数値なので[1]とかない→多分n*nの解像度（画像サイズ）表示じゃなくて相対解像度??\n",
    "        #とりあえずimage_size // patch_sizeにした\n",
    "        \n",
    "        # num_patches = config.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        # modality type embedding\n",
    "        self.token_type_embeddings = nn.Embedding(config.modality_type_vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.config = config\n",
    "\n",
    "    def visual_embed(self,pixel_values, pixel_mask, max_image_length=200):\n",
    "        _, _, ph, pw = self.patch_embeddings.projection.weight.shape\n",
    "        print('x: {x.size()}')\n",
    "\n",
    "        x = self.patch_embeddings(pixel_values)\n",
    "        print(f'x: {x.size()}')\n",
    "        x_mask = pixel_mask[:, None, :, :].float()\n",
    "        print(f'x_mask: {x_mask.size()}')\n",
    "        x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "        x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "        x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        patch_dim = self.config.image_size // self.config.patch_size\n",
    "        spatial_pos = self.position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "        pos_embed = torch.cat(\n",
    "            [\n",
    "                nn.functional.pad(\n",
    "                    nn.functional.interpolate(\n",
    "                        spatial_pos,\n",
    "                        size=(h, w),\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=True,\n",
    "                    ),\n",
    "                    (0, width - w, 0, height - h),\n",
    "                )\n",
    "                for h, w in zip(x_h, x_w)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "         # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "        patch_index = torch.stack(\n",
    "            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "        ).to(device=x_mask.device)\n",
    "        patch_index = patch_index[None, None, :, :, :]\n",
    "        patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "        patch_index = patch_index.flatten(1, 3)\n",
    "        x_mask = x_mask.flatten(1)\n",
    "\n",
    "        if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "            # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "            # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "            # if self.patch_size = 32, 25 * 41 = 1025\n",
    "            # if res is 384 x 640, 12 * 20 = 240\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = effective_resolution.max()\n",
    "        else:\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "        valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "        non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "        unique_rows = valid_idx[:, 0].unique()\n",
    "        valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "        non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "        pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "        select = list()\n",
    "        for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "            if p <= 0:\n",
    "                valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "                select.append(valid_row_idx[i][valid_choice])\n",
    "            else:\n",
    "                pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "                select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "        select = torch.cat(select, dim=0)\n",
    "        x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "        x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "        # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "        patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "        pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        pos_embed = torch.cat(\n",
    "            (self.position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "        )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "        return x, x_mask, (patch_index, (height, width))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask,image_token_type_idx=1):\n",
    "            # 1. text embeddings\n",
    "            text_embeds = self.text_embeddings(\n",
    "                input_ids = input_ids, token_type_ids= token_type_ids )\n",
    "\n",
    "            # 2. patch embeddings\n",
    "            \"\"\"if use clip, change code here\n",
    "            for example: \n",
    "                import clip\n",
    "                model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "                image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "                image_embeds = model.encode_image(image)\n",
    "            \"\"\"\n",
    "            image_embeds, image_masks, patch_index = self.visual_embed(\n",
    "                pixel_values , pixel_mask, max_image_length=self.config.max_image_length )\n",
    "            \n",
    "            # 3. add modality type embedding\n",
    "            # text_embeds = text_embeds + self.token_type_embeddings(\n",
    "            #     torch.zeros_like(attention_mask,dtype=torch.long, device = text_embeds.device))\n",
    "\n",
    "            image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx, dtype=torch.long, device=image_embeds.device))\n",
    "\n",
    "            # 4. concat\n",
    "            embeddings = torch.cat([text_embeds, image_embeds], dim =1)\n",
    "            masks = torch.cat([attention_mask, image_masks], dim=1)\n",
    "            return embeddings, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62928fca7e1f411ea4a689dd797050bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "from transformer_block import MultiHeadAttention\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "config = ViltConfig()\n",
    "# model = ViltModel(config)\n",
    "path1 = 'Dataset/questions/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "path2 = 'Dataset/annotations/v2_mscoco_val2014_annotations.json'\n",
    "questions, annotations = read_data(path1, path2, config)\n",
    "# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "dataset = VQADataset(questions[64:128], annotations[64:128], config)\n",
    "dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "# for datas in tqdm(dataloader):\n",
    "#     data = {k: v for k, v in datas.items()}\n",
    "#     data.pop('labels')\n",
    "#     print(data.keys())\n",
    "# data\n",
    "data = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 608, 544])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataiter = iter(dataloader)\n",
    "# data1=next(dataiter)\n",
    "data[\"pixel_values\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, pooled_output, output = model(**data)\n",
    "\n",
    "print(encoder_output, pooled_output, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visual_embed(pixel_values, pixel_mask, max_image_length=200):\n",
    "    patch_embeddings = PatchEmbeddings(config)\n",
    "    dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    print(pixel_values.size())\n",
    "    cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "    num_patches = patch_embeddings.num_patches\n",
    "    position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "    _, _, ph, pw = patch_embeddings.projection.weight.shape\n",
    "\n",
    "    x = patch_embeddings(pixel_values)\n",
    "    print(x.size())\n",
    "    x_mask = pixel_mask[:, None, :, :].float()\n",
    "    x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "    x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "    x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    patch_dim = config.image_size // config.patch_size\n",
    "    spatial_pos = position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "    pos_embed = torch.cat(\n",
    "        [\n",
    "            nn.functional.pad(\n",
    "                nn.functional.interpolate(\n",
    "                    spatial_pos,\n",
    "                    size=(h, w),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                ),\n",
    "                (0, width - w, 0, height - h),\n",
    "            )\n",
    "            for h, w in zip(x_h, x_w)\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "        # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "    patch_index = torch.stack(\n",
    "        torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "    ).to(device=x_mask.device)\n",
    "    patch_index = patch_index[None, None, :, :, :]\n",
    "    patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "    patch_index = patch_index.flatten(1, 3)\n",
    "    x_mask = x_mask.flatten(1)\n",
    "\n",
    "    if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "        # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "        # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "        # if self.patch_size = 32, 25 * 41 = 1025\n",
    "        # if res is 384 x 640, 12 * 20 = 240\n",
    "        effective_resolution = x_h * x_w\n",
    "        max_image_length = effective_resolution.max()\n",
    "    else:\n",
    "        effective_resolution = x_h * x_w\n",
    "        max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "    valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "    non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "    unique_rows = valid_idx[:, 0].unique()\n",
    "    valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "    non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "    valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "    non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "    pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "    select = list()\n",
    "    for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "        if p <= 0:\n",
    "            valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "            select.append(valid_row_idx[i][valid_choice])\n",
    "        else:\n",
    "            pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "            select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "    select = torch.cat(select, dim=0)\n",
    "    x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "    x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "    # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "    patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "    pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "    cls_tokens = cls_token.expand(batch_size, -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "    pos_embed = torch.cat(\n",
    "        (position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "    )\n",
    "    x = x + pos_embed\n",
    "    x = dropout(x)\n",
    "\n",
    "    x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "    return x, x_mask, (patch_index, (height, width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 608, 544])\n",
      "torch.Size([32, 768, 19, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-4.7247e-01,  1.2036e+00, -1.2913e+00,  ..., -3.9468e-01,\n",
       "           -4.0587e-01, -6.7620e-01],\n",
       "          [-2.2681e-01,  1.3573e+00, -1.2458e+00,  ..., -3.5481e-01,\n",
       "           -2.2803e-01, -6.5137e-01],\n",
       "          ...,\n",
       "          [-1.0024e-02,  1.7419e-02, -1.3389e-02,  ...,  1.1874e-02,\n",
       "            2.6462e-03, -1.5969e-03],\n",
       "          [-1.0024e-02,  1.7419e-02, -1.3389e-02,  ...,  1.1874e-02,\n",
       "            2.6462e-03, -1.5969e-03],\n",
       "          [-1.0024e-02,  1.7419e-02, -1.3389e-02,  ...,  1.1874e-02,\n",
       "            2.6462e-03, -1.5969e-03]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.8138e-01,  3.1945e-01, -3.1967e-01,  ..., -1.0940e-01,\n",
       "           -2.5953e-01, -4.1980e-01],\n",
       "          [ 1.5408e-01, -7.9162e-01,  9.2724e-01,  ...,  2.2069e-01,\n",
       "            3.5936e-02,  3.8392e-01],\n",
       "          ...,\n",
       "          [-1.2180e-01, -6.1510e-01,  5.7102e-01,  ...,  2.2927e-01,\n",
       "            5.4089e-02,  1.7050e-01],\n",
       "          [-2.6585e-01,  3.5342e-01, -3.6650e-01,  ..., -1.0348e-01,\n",
       "           -2.6559e-01, -4.4200e-01],\n",
       "          [ 2.1354e-01, -3.0134e-01,  4.8516e-01,  ...,  2.4228e-01,\n",
       "            1.1619e-01,  5.1860e-02]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.5570e-01,  3.7243e-01, -3.4765e-01,  ..., -2.7493e-01,\n",
       "            5.9042e-01, -7.8993e-02],\n",
       "          [-4.7750e-01,  8.3740e-01, -8.1719e-01,  ..., -3.1681e-01,\n",
       "            3.5728e-01, -3.3472e-01],\n",
       "          ...,\n",
       "          [-1.0024e-02,  1.7419e-02, -1.3389e-02,  ...,  1.1874e-02,\n",
       "            2.6462e-03, -1.5969e-03],\n",
       "          [-1.0024e-02,  1.7419e-02, -1.3389e-02,  ...,  1.1874e-02,\n",
       "            2.6462e-03, -1.5969e-03],\n",
       "          [-1.0024e-02,  1.7419e-02, -1.3389e-02,  ...,  1.1874e-02,\n",
       "            2.6462e-03, -1.5969e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.7866e-01,  2.5290e-01, -2.8902e-01,  ..., -8.7923e-02,\n",
       "           -3.3632e-01, -4.4931e-01],\n",
       "          [ 2.4392e-01, -1.0478e+00,  9.3554e-01,  ...,  5.1859e-01,\n",
       "            1.3979e-01,  4.4445e-01],\n",
       "          ...,\n",
       "          [-2.8224e-01,  1.9312e-01, -2.4449e-01,  ..., -6.1960e-02,\n",
       "           -2.3559e-01, -4.0735e-01],\n",
       "          [ 9.6503e-02, -8.7877e-01,  9.1408e-01,  ...,  3.1726e-01,\n",
       "            1.9838e-02,  3.7560e-01],\n",
       "          [ 1.0325e-01, -4.9392e-01,  5.3919e-01,  ...,  1.0828e-01,\n",
       "            7.5874e-02,  3.2919e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 3.8421e-02,  3.8479e-01, -4.0711e-01,  ..., -3.7431e-02,\n",
       "           -4.1879e-01, -7.5251e-01],\n",
       "          [-1.1411e-01, -8.8256e-01,  9.2506e-01,  ...,  2.4487e-01,\n",
       "            3.6317e-01,  3.7886e-01],\n",
       "          ...,\n",
       "          [ 2.4392e-01, -1.0478e+00,  9.3554e-01,  ...,  5.1859e-01,\n",
       "            1.3979e-01,  4.4445e-01],\n",
       "          [-3.2084e-01,  1.5646e-01, -2.8639e-01,  ..., -6.4908e-02,\n",
       "           -2.3016e-01, -4.1745e-01],\n",
       "          [-8.2837e-04, -1.1071e-01,  4.0303e-01,  ...,  2.4869e-01,\n",
       "            1.7106e-01,  3.7638e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-3.4048e-01,  2.5120e-01, -6.3737e-01,  ...,  1.5225e-01,\n",
       "           -1.3228e-01, -5.8083e-01],\n",
       "          [-7.9497e-03, -4.2668e-01,  7.0459e-01,  ...,  1.8898e-01,\n",
       "           -1.7818e-02,  6.2766e-02],\n",
       "          ...,\n",
       "          [-3.4050e-01,  5.1912e-01, -5.5978e-01,  ..., -1.4885e-01,\n",
       "           -2.8386e-01, -5.3665e-01],\n",
       "          [-2.4370e-01,  3.0942e-01, -3.2560e-01,  ..., -8.7464e-02,\n",
       "           -2.6063e-01, -4.0833e-01],\n",
       "          [-2.8959e-01,  4.0318e-01, -3.7323e-01,  ..., -1.3579e-01,\n",
       "           -3.0505e-01, -4.6711e-01]]], grad_fn=<AddBackward0>),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " (tensor([[[ 0,  0],\n",
       "           [ 0,  1],\n",
       "           [ 0,  2],\n",
       "           ...,\n",
       "           [13, 15],\n",
       "           [15,  8],\n",
       "           [15, 10]],\n",
       "  \n",
       "          [[ 3,  5],\n",
       "           [14,  6],\n",
       "           [ 7,  7],\n",
       "           ...,\n",
       "           [17,  9],\n",
       "           [11, 10],\n",
       "           [11,  6]],\n",
       "  \n",
       "          [[ 0,  0],\n",
       "           [ 0,  1],\n",
       "           [ 0,  2],\n",
       "           ...,\n",
       "           [ 0, 16],\n",
       "           [15,  9],\n",
       "           [16,  9]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 2,  0],\n",
       "           [ 8,  9],\n",
       "           [ 3, 12],\n",
       "           ...,\n",
       "           [ 0, 11],\n",
       "           [11, 10],\n",
       "           [ 6, 15]],\n",
       "  \n",
       "          [[ 5,  0],\n",
       "           [ 9,  7],\n",
       "           [10, 15],\n",
       "           ...,\n",
       "           [ 8,  9],\n",
       "           [ 1, 13],\n",
       "           [ 5, 14]],\n",
       "  \n",
       "          [[11,  8],\n",
       "           [15,  3],\n",
       "           [ 6,  3],\n",
       "           ...,\n",
       "           [ 5,  9],\n",
       "           [ 2,  8],\n",
       "           [ 9,  9]]]),\n",
       "  (19, 17)))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vilt_model import ViltEmbeddings\n",
    "s= data\n",
    "\n",
    "visual_embed(s[\"pixel_values\"],s[\"pixel_mask\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())\n",
    "# last_hidden_state = outputs.last_hidden_state\n",
    "# pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa38bdac91f012d3aa63752192202c1c4a31351885bf71923469150acc0ed662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
