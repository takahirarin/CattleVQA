{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "#from transformer_block import MultiHeadAttention, FFN, SubLayerConnection\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "from transformers import logging\n",
    "from transformers import logging\n",
    "\n",
    "\n",
    "\n",
    "class ViltModel(nn.Module):\n",
    "    def __init__(self, config: ViltConfig):\n",
    "        \"\"\"\"config: 'ViltConfig' instance \"\"\"\n",
    "        super(ViltModel, self).__init__()\n",
    "        self.embeddings = ViltEmbeddings(config)\n",
    "        self.encoder = ViltEncoder(config)\n",
    "        self.pooler = ViltPooler(config)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.classifier = ViltClassifer(config)\n",
    "\n",
    "\n",
    "    def get_extended_attention_mask(self, attention_mask, input_shape, device):\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            # if config.is_decoder:\n",
    "            #     batch_size, seq_length = input_shape\n",
    "            #     seq_ids = torch.arange(seq_length, device=device)\n",
    "            #     causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "            #     # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "            #     # causal and attention masks must have same type with pytorch version < 1.3\n",
    "            #     causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "            #     if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "            #         prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "            #         causal_mask = torch.cat(\n",
    "            #             [\n",
    "            #                 torch.ones(\n",
    "            #                     (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
    "            #                 ),\n",
    "            #                 causal_mask,\n",
    "            #             ],\n",
    "            #             axis=-1,\n",
    "            #         )\n",
    "\n",
    "            #     extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            # else:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        #extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask, image_token_type_idx=1):\n",
    "        input_shape = input_ids.size()\n",
    "        # get text info\n",
    "        text_batch_size, seq_length = input_shape\n",
    "        device = input_ids.device\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((text_batch_size, seq_length)), device=device)\n",
    "        # get image info\n",
    "        image_batch_size =  pixel_values.shape[0] \n",
    "        if pixel_values is None:\n",
    "            pixel_values = torch.ones((image_batch_size, self.config.image_size, self.config_image_size), device=device)\n",
    "        \n",
    "        # calculate embeddings\n",
    "        embeddings, masks = self.embeddings(\n",
    "            input_ids, attention_mask, token_type_ids,\n",
    "            pixel_values, pixel_mask,image_token_type_idx )\n",
    "        \n",
    "        # input embeddings into encoder\n",
    "        # extended_attention_mask = ModuleUtilsMixin.get_extended_attention_mask(attention_mask, input_shape)\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(masks, input_shape, device)\n",
    "        encoder_output = self.encoder(embeddings, extended_attention_mask)\n",
    "        sequence_output = encoder_output[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        #classifier\n",
    "        output = self.classifier(pooled_output)\n",
    "        return encoder_output, pooled_output, output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ViltClassifer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(config.hidden_size, config.hidden_size*2)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size*2)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.fc(x)\n",
    "        output = self.norm(output)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self,config) :\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        layer = ViltLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers\n",
    "\n",
    "\n",
    "\n",
    "class ViltLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.transformer_block = FFN(config)\n",
    "        self.shortcut = SubLayerConnection(config)\n",
    "    def forward(self,hidden_states,attention_mask):\n",
    "        attention_output = self.attention(hidden_states, hidden_states, hidden_states,\n",
    "            config.num_attention_heads, config.hidden_size, attention_mask)\n",
    "        FFN_output = self.transformer_block(attention_output)\n",
    "        layer_output = self.shortcut(FFN_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "class ViltPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"taking the hidden state corresponding to the first token.\"\"\"\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class ViltEmbeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        logging.set_verbosity_error()\n",
    "        logging.set_verbosity_warning()\n",
    "        #super(ViltEmbeddings).__init__()\n",
    "        super().__init__()\n",
    "         # text embeddings\n",
    "        self.text_embeddings = BERTEmbeddings(config)\n",
    "        # patch embeddings\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.vision_embeddings = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        \n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        # modality type embedding\n",
    "        self.token_type_embeddings = nn.Embedding(config.modality_type_vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.config = config\n",
    "\n",
    "    def mask_embed(self,pixel_values, pixel_mask, max_image_length=200):\n",
    "        _, _, ph, pw = self.patch_embeddings.projection.weight.shape\n",
    "\n",
    "        x = self.patch_embeddings(pixel_values)\n",
    "        x_mask = pixel_mask[:, None, :, :].float()\n",
    "        x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "        x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "        x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        patch_dim = self.config.image_size // self.config.patch_size\n",
    "        spatial_pos = self.position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "        pos_embed = torch.cat(\n",
    "            [\n",
    "                nn.functional.pad(\n",
    "                    nn.functional.interpolate(\n",
    "                        spatial_pos,\n",
    "                        size=(h, w),\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=True,\n",
    "                    ),\n",
    "                    (0, width - w, 0, height - h),\n",
    "                )\n",
    "                for h, w in zip(x_h, x_w)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "         # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "        patch_index = torch.stack(\n",
    "            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "        ).to(device=x_mask.device)\n",
    "        patch_index = patch_index[None, None, :, :, :]\n",
    "        patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "        patch_index = patch_index.flatten(1, 3)\n",
    "        x_mask = x_mask.flatten(1)\n",
    "\n",
    "        if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "            # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "            # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "            # if self.patch_size = 32, 25 * 41 = 1025\n",
    "            # if res is 384 x 640, 12 * 20 = 240\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = effective_resolution.max()\n",
    "        else:\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "        valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "        non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "        unique_rows = valid_idx[:, 0].unique()\n",
    "        valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "        non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "        pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "        select = list()\n",
    "        for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "            if p <= 0:\n",
    "                valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "                select.append(valid_row_idx[i][valid_choice])\n",
    "            else:\n",
    "                pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "                select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "        select = torch.cat(select, dim=0)\n",
    "        x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "        x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "        # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "        patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "        pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        pos_embed = torch.cat(\n",
    "            (self.position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "        )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "        #return x, x_mask, (patch_index, (height, width))\n",
    "        return x_mask\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask,image_token_type_idx=1):\n",
    "            # 1. text embeddings\n",
    "            text_embeds = self.text_embeddings(\n",
    "                input_ids = input_ids, token_type_ids= token_type_ids )\n",
    "\n",
    "            # 2. patch embeddings\n",
    "            \"\"\"if use clip, change code here\n",
    "            for example: \n",
    "                import clip\n",
    "                model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "                image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "                image_embeds = model.encode_image(image)\n",
    "\n",
    "                モジュール自体を少し変更しました\n",
    "            \"\"\"\n",
    "            not_use, image_embeds,  = self.vision_embeddings(pixel_values)\n",
    "\n",
    "            image_masks = self.mask_embed(pixel_values , pixel_mask, max_image_length=self.config.max_image_length)\n",
    "            \n",
    "            # 3. add modality type embedding\n",
    "            # text_embeds = text_embeds + self.token_type_embeddings(\n",
    "            #     torch.zeros_like(attention_mask,dtype=torch.long, device = text_embeds.device))\n",
    "\n",
    "            image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx, dtype=torch.long, device=image_embeds.device))\n",
    "\n",
    "            # 4. concat\n",
    "            embeddings = torch.cat([text_embeds, image_embeds], dim =1)\n",
    "            masks = torch.cat([attention_mask, image_masks], dim=1)\n",
    "            return embeddings, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from vilt_config import ViltConfig\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "            super(Attention, self).__init__()\n",
    "            \n",
    "    def forward(self, query, key, value, attention_mask,attention_dropout_prob):\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(query.size(-1)) #(query * key^T) / √d_k\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "            #0のところを∞に置き換えてマスクする？\n",
    "\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        #行方向にsoftmax\n",
    "\n",
    "        if attention_dropout_prob is not None:\n",
    "            attn = attention_dropout_prob(scores)\n",
    "        \n",
    "        # attn: [batch_size, head, 1, 1], value: [batch_size, head, 1, d_k]\n",
    "        # attn*value = [batch_size, head, 1, d_k]\n",
    "        return torch.matmul(attn, value), attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        #super(MultiHeadAttention).__init__()\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(config.hidden_size, config.hidden_size) for _ in range(config.num_attention_heads)])\n",
    "        self.output_linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attention = Attention(config)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,query, key, value, head, hidden_size, attention_mask=None, attention_dropout_prob=None):\n",
    "        batch_size = query.size(0)\n",
    "        d_k = hidden_size // head\n",
    "\n",
    "        query, key, value = [l(x).view(batch_size, -1, head, d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "                            #h個のattention層に分けるために(batch_size, -1, head, d_k)の形にする\n",
    "        \n",
    "        attention_output, attn = self.attention(query, key, value, attention_mask, attention_dropout_prob)\n",
    "        \n",
    "        attention_output = attention_output.transpose(1,2).contiguous().view(batch_size, -1, head * d_k)\n",
    "        #41行目でtranspose(1,2)してたので戻すためのtranspose\n",
    "        #viewを使うときは要素順に並んでいないといけないのでそのためのcontiguous()\n",
    "        #[batch_size, 1, hidden_size] で出力\n",
    "\n",
    "\n",
    "        return self.output_linear(attention_output)\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(FFN,self).__init__()\n",
    "        self.dense  = nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        self.activation = nn.GELU()\n",
    "        #configから文字じゃなくてメソッドを呼び起こす方法がわからない\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "class SubLayerConnection(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(SubLayerConnection,self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,hidden_states,input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2591798ed515465b9cf8473e8bc782b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "from transformer_block import MultiHeadAttention\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "config = ViltConfig()\n",
    "# model = ViltModel(config)\n",
    "path1 = 'Dataset/questions/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "path2 = 'Dataset/annotations/v2_mscoco_val2014_annotations.json'\n",
    "questions, annotations = read_data(path1, path2, config)\n",
    "# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "dataset = VQADataset(questions, annotations, config)\n",
    "dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "# for datas in tqdm(dataloader):\n",
    "#     data = {k: v for k, v in datas.items()}\n",
    "#     data.pop('labels')\n",
    "#     print(data.keys())\n",
    "# data\n",
    "data = next(iter(dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for datas in tqdm(dataloader):\n",
    "#     data = {k: v for k, v in datas.items()}\n",
    "#     data.pop('labels')\n",
    "#     print(data.keys())\n",
    "#     break\n",
    "\n",
    "data.pop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'logit_scale', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'visual_projection.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([[[ 5.2070e-01,  7.8241e-02,  2.1578e-02,  ..., -1.0004e+00,\n",
      "           7.8847e-01, -1.4449e-01],\n",
      "         [ 4.6402e-01,  5.6579e-02, -4.4090e-02,  ..., -9.6672e-01,\n",
      "           7.1919e-01, -5.7584e-02],\n",
      "         [ 5.3666e-01,  8.8132e-02,  1.0509e-01,  ..., -1.0184e+00,\n",
      "           8.7418e-01, -2.0717e-01],\n",
      "         ...,\n",
      "         [ 3.2085e-01,  1.2301e-01,  4.5868e-01,  ..., -7.2930e-01,\n",
      "           7.5149e-01, -1.1620e-01],\n",
      "         [ 3.1926e-01,  1.2297e-01,  4.2714e-01,  ..., -7.3103e-01,\n",
      "           7.6217e-01, -1.4177e-01],\n",
      "         [ 3.2820e-01,  1.2701e-01,  4.0288e-01,  ..., -7.5151e-01,\n",
      "           7.5883e-01, -1.3288e-01]],\n",
      "\n",
      "        [[-1.1688e-01, -3.6910e-01,  1.2062e+00,  ..., -5.1490e-01,\n",
      "           1.5050e-01, -2.5467e-01],\n",
      "         [-1.6941e-01, -4.4079e-01,  1.2109e+00,  ..., -4.3346e-01,\n",
      "           4.2583e-02, -1.7279e-01],\n",
      "         [-1.9778e-01, -3.3443e-01,  1.1819e+00,  ..., -5.6574e-01,\n",
      "           3.0406e-01, -1.8998e-01],\n",
      "         ...,\n",
      "         [-1.9342e-01, -1.6191e-01,  1.6032e+00,  ..., -3.1200e-01,\n",
      "           1.9329e-01, -3.5117e-01],\n",
      "         [-2.1129e-01, -1.9767e-01,  1.5216e+00,  ..., -3.3096e-01,\n",
      "           1.7686e-01, -2.8627e-01],\n",
      "         [-2.1465e-01, -2.1908e-01,  1.5377e+00,  ..., -3.4730e-01,\n",
      "           1.9081e-01, -2.7609e-01]],\n",
      "\n",
      "        [[ 2.9619e-01, -3.6225e-01,  1.9066e+00,  ..., -4.1616e-01,\n",
      "           1.3290e-02, -9.2682e-02],\n",
      "         [ 2.3993e-01, -4.1612e-01,  1.7303e+00,  ..., -3.4758e-01,\n",
      "          -1.1675e-01, -2.0662e-02],\n",
      "         [ 2.0299e-01, -3.2780e-01,  1.7959e+00,  ..., -5.6010e-01,\n",
      "           1.7441e-01, -8.3029e-03],\n",
      "         ...,\n",
      "         [ 7.3339e-02, -2.4414e-01,  2.2257e+00,  ..., -2.6487e-01,\n",
      "           1.5128e-01, -1.4714e-01],\n",
      "         [ 1.6348e-01, -1.8299e-01,  2.0790e+00,  ..., -4.1736e-01,\n",
      "           7.2647e-02, -1.5312e-02],\n",
      "         [ 2.3783e-01, -3.9884e-01,  2.3622e+00,  ..., -1.9594e-01,\n",
      "           2.0080e-02, -1.5944e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3130e-01, -3.4259e-01,  7.6068e-01,  ..., -7.5660e-01,\n",
      "           2.4614e-01, -7.4442e-01],\n",
      "         [ 2.6816e-02, -3.7377e-01,  6.5910e-01,  ..., -7.2033e-01,\n",
      "           1.3499e-01, -6.3415e-01],\n",
      "         [ 4.4705e-02, -2.6158e-01,  6.7390e-01,  ..., -8.9341e-01,\n",
      "           3.4374e-01, -6.6223e-01],\n",
      "         ...,\n",
      "         [ 4.4372e-02, -2.1369e-01,  9.0285e-01,  ..., -7.1923e-01,\n",
      "           2.0684e-01, -7.1408e-01],\n",
      "         [ 4.9283e-02, -2.2964e-01,  9.0105e-01,  ..., -7.1949e-01,\n",
      "           2.0613e-01, -7.1085e-01],\n",
      "         [ 3.9844e-02, -2.1219e-01,  9.2179e-01,  ..., -7.1431e-01,\n",
      "           2.1093e-01, -7.1146e-01]],\n",
      "\n",
      "        [[ 3.3399e-02, -1.9065e-01,  1.0268e+00,  ..., -6.5627e-01,\n",
      "           1.9633e-01, -2.5975e-01],\n",
      "         [-3.9377e-02, -1.9635e-01,  9.2286e-01,  ..., -5.6463e-01,\n",
      "           4.1257e-02, -2.0342e-01],\n",
      "         [-7.9273e-04, -1.1010e-01,  1.0134e+00,  ..., -6.9965e-01,\n",
      "           3.3156e-01, -2.1219e-01],\n",
      "         ...,\n",
      "         [-1.4370e-01, -5.4812e-03,  1.1632e+00,  ..., -3.3703e-01,\n",
      "           1.8626e-02, -1.8655e-01],\n",
      "         [-3.8495e-02,  2.3842e-01,  1.1413e+00,  ..., -4.6056e-01,\n",
      "           4.5043e-01, -1.9247e-01],\n",
      "         [-9.1419e-03,  1.3111e-01,  1.4418e+00,  ..., -4.2295e-01,\n",
      "           2.8737e-01, -5.0413e-01]],\n",
      "\n",
      "        [[-1.3225e-02, -2.4559e-01,  1.0815e+00,  ..., -4.8395e-01,\n",
      "           9.6947e-02, -4.7871e-01],\n",
      "         [-3.5183e-02, -2.8305e-01,  1.1853e+00,  ..., -4.3025e-01,\n",
      "           2.3056e-02, -4.9279e-01],\n",
      "         [ 1.0929e-02, -2.2814e-01,  1.1338e+00,  ..., -6.6313e-01,\n",
      "           2.1835e-01, -5.7230e-01],\n",
      "         ...,\n",
      "         [-1.7359e-01, -1.5058e-01,  1.4301e+00,  ..., -2.6019e-01,\n",
      "           7.7083e-02, -4.7599e-01],\n",
      "         [-1.9655e-01, -9.6319e-02,  1.3668e+00,  ..., -2.5034e-01,\n",
      "           8.5061e-02, -4.9881e-01],\n",
      "         [-1.4352e-01, -2.4657e-01,  1.6662e+00,  ..., -1.7399e-01,\n",
      "          -1.2891e-02, -5.8378e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2814,  1.0222, -0.7167,  ..., -0.6033, -1.7535, -0.8065],\n",
      "         [ 0.2811,  1.0233, -0.7165,  ..., -0.6055, -1.7544, -0.8042],\n",
      "         [ 0.2819,  1.0212, -0.7165,  ..., -0.6029, -1.7531, -0.8088],\n",
      "         ...,\n",
      "         [ 0.2810,  1.0201, -0.7147,  ..., -0.6031, -1.7559, -0.8076],\n",
      "         [ 0.2808,  1.0206, -0.7148,  ..., -0.6033, -1.7559, -0.8069],\n",
      "         [ 0.2808,  1.0206, -0.7150,  ..., -0.6033, -1.7558, -0.8070]],\n",
      "\n",
      "        [[-0.0474, -0.0958, -0.6301,  ..., -0.3521, -1.1994, -0.8278],\n",
      "         [-0.0483, -0.0968, -0.6301,  ..., -0.3514, -1.2000, -0.8280],\n",
      "         [-0.0473, -0.0964, -0.6302,  ..., -0.3529, -1.2003, -0.8277],\n",
      "         ...,\n",
      "         [-0.0476, -0.0933, -0.6306,  ..., -0.3495, -1.2033, -0.8298],\n",
      "         [-0.0478, -0.0940, -0.6305,  ..., -0.3500, -1.2026, -0.8293],\n",
      "         [-0.0478, -0.0942, -0.6305,  ..., -0.3501, -1.2027, -0.8293]],\n",
      "\n",
      "        [[ 0.1434,  0.2702, -0.5119,  ..., -0.2904, -1.3457, -0.6847],\n",
      "         [ 0.1428,  0.2699, -0.5117,  ..., -0.2912, -1.3450, -0.6847],\n",
      "         [ 0.1434,  0.2689, -0.5116,  ..., -0.2915, -1.3461, -0.6856],\n",
      "         ...,\n",
      "         [ 0.1429,  0.2690, -0.5129,  ..., -0.2895, -1.3472, -0.6847],\n",
      "         [ 0.1433,  0.2694, -0.5125,  ..., -0.2906, -1.3458, -0.6843],\n",
      "         [ 0.1426,  0.2689, -0.5114,  ..., -0.2893, -1.3483, -0.6859]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2824,  0.5928, -0.5338,  ..., -0.5808, -1.3829, -0.7757],\n",
      "         [ 0.2813,  0.5924, -0.5334,  ..., -0.5811, -1.3834, -0.7745],\n",
      "         [ 0.2820,  0.5925, -0.5342,  ..., -0.5817, -1.3835, -0.7758],\n",
      "         ...,\n",
      "         [ 0.2813,  0.5937, -0.5335,  ..., -0.5819, -1.3852, -0.7762],\n",
      "         [ 0.2814,  0.5937, -0.5335,  ..., -0.5817, -1.3851, -0.7763],\n",
      "         [ 0.2812,  0.5937, -0.5335,  ..., -0.5817, -1.3853, -0.7763]],\n",
      "\n",
      "        [[ 0.3753,  0.5303, -0.6167,  ..., -0.1188, -1.4526, -0.9651],\n",
      "         [ 0.3737,  0.5294, -0.6160,  ..., -0.1180, -1.4541, -0.9651],\n",
      "         [ 0.3751,  0.5300, -0.6164,  ..., -0.1199, -1.4536, -0.9653],\n",
      "         ...,\n",
      "         [ 0.3748,  0.5307, -0.6152,  ..., -0.1171, -1.4544, -0.9668],\n",
      "         [ 0.3750,  0.5313, -0.6144,  ..., -0.1233, -1.4568, -0.9623],\n",
      "         [ 0.3737,  0.5339, -0.6156,  ..., -0.1193, -1.4556, -0.9629]],\n",
      "\n",
      "        [[ 0.2212,  0.3864, -0.6925,  ..., -0.2122, -1.5024, -0.7710],\n",
      "         [ 0.2199,  0.3857, -0.6917,  ..., -0.2122, -1.5036, -0.7702],\n",
      "         [ 0.2214,  0.3862, -0.6933,  ..., -0.2119, -1.5021, -0.7742],\n",
      "         ...,\n",
      "         [ 0.2201,  0.3858, -0.6929,  ..., -0.2110, -1.5043, -0.7721],\n",
      "         [ 0.2197,  0.3864, -0.6924,  ..., -0.2081, -1.5047, -0.7693],\n",
      "         [ 0.2201,  0.3844, -0.6927,  ..., -0.2096, -1.5051, -0.7735]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.3341e+00,  6.5606e-01, -7.7672e-02,  ..., -6.9044e-01,\n",
      "           2.1355e+00,  1.7235e+00],\n",
      "         [-1.3341e+00,  6.5606e-01, -7.7671e-02,  ..., -6.9044e-01,\n",
      "           2.1355e+00,  1.7235e+00],\n",
      "         [-1.3341e+00,  6.5606e-01, -7.7672e-02,  ..., -6.9044e-01,\n",
      "           2.1355e+00,  1.7235e+00],\n",
      "         ...,\n",
      "         [-1.3341e+00,  6.5606e-01, -7.7672e-02,  ..., -6.9044e-01,\n",
      "           2.1355e+00,  1.7235e+00],\n",
      "         [-1.3341e+00,  6.5606e-01, -7.7672e-02,  ..., -6.9044e-01,\n",
      "           2.1355e+00,  1.7235e+00],\n",
      "         [-1.3341e+00,  6.5606e-01, -7.7672e-02,  ..., -6.9044e-01,\n",
      "           2.1355e+00,  1.7235e+00]],\n",
      "\n",
      "        [[-4.9963e-01, -4.5374e-01,  5.0752e-02,  ..., -1.1833e+00,\n",
      "           1.5730e+00,  2.4633e+00],\n",
      "         [-4.9963e-01, -4.5374e-01,  5.0751e-02,  ..., -1.1833e+00,\n",
      "           1.5730e+00,  2.4633e+00],\n",
      "         [-4.9963e-01, -4.5374e-01,  5.0752e-02,  ..., -1.1833e+00,\n",
      "           1.5730e+00,  2.4633e+00],\n",
      "         ...,\n",
      "         [-4.9963e-01, -4.5374e-01,  5.0752e-02,  ..., -1.1833e+00,\n",
      "           1.5730e+00,  2.4633e+00],\n",
      "         [-4.9963e-01, -4.5374e-01,  5.0751e-02,  ..., -1.1833e+00,\n",
      "           1.5730e+00,  2.4633e+00],\n",
      "         [-4.9963e-01, -4.5374e-01,  5.0752e-02,  ..., -1.1833e+00,\n",
      "           1.5730e+00,  2.4633e+00]],\n",
      "\n",
      "        [[-5.6189e-01, -4.8932e-01, -3.0711e-01,  ..., -6.4469e-01,\n",
      "           1.2395e+00,  2.0473e+00],\n",
      "         [-5.6189e-01, -4.8932e-01, -3.0711e-01,  ..., -6.4469e-01,\n",
      "           1.2395e+00,  2.0473e+00],\n",
      "         [-5.6189e-01, -4.8932e-01, -3.0711e-01,  ..., -6.4469e-01,\n",
      "           1.2395e+00,  2.0473e+00],\n",
      "         ...,\n",
      "         [-5.6189e-01, -4.8932e-01, -3.0711e-01,  ..., -6.4469e-01,\n",
      "           1.2395e+00,  2.0473e+00],\n",
      "         [-5.6189e-01, -4.8932e-01, -3.0711e-01,  ..., -6.4469e-01,\n",
      "           1.2395e+00,  2.0473e+00],\n",
      "         [-5.6189e-01, -4.8932e-01, -3.0711e-01,  ..., -6.4469e-01,\n",
      "           1.2395e+00,  2.0473e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.8528e-01, -1.5678e-01,  3.2328e-04,  ..., -1.0701e+00,\n",
      "           1.5881e+00,  2.2849e+00],\n",
      "         [-6.8528e-01, -1.5678e-01,  3.2374e-04,  ..., -1.0701e+00,\n",
      "           1.5881e+00,  2.2849e+00],\n",
      "         [-6.8528e-01, -1.5678e-01,  3.2405e-04,  ..., -1.0701e+00,\n",
      "           1.5881e+00,  2.2849e+00],\n",
      "         ...,\n",
      "         [-6.8528e-01, -1.5678e-01,  3.2356e-04,  ..., -1.0701e+00,\n",
      "           1.5881e+00,  2.2849e+00],\n",
      "         [-6.8528e-01, -1.5678e-01,  3.2357e-04,  ..., -1.0701e+00,\n",
      "           1.5881e+00,  2.2849e+00],\n",
      "         [-6.8528e-01, -1.5678e-01,  3.2359e-04,  ..., -1.0701e+00,\n",
      "           1.5881e+00,  2.2849e+00]],\n",
      "\n",
      "        [[-5.4404e-01, -1.9802e-01,  9.2835e-02,  ..., -1.0418e+00,\n",
      "           1.1960e+00,  1.9206e+00],\n",
      "         [-5.4404e-01, -1.9802e-01,  9.2835e-02,  ..., -1.0418e+00,\n",
      "           1.1960e+00,  1.9206e+00],\n",
      "         [-5.4404e-01, -1.9802e-01,  9.2835e-02,  ..., -1.0418e+00,\n",
      "           1.1960e+00,  1.9206e+00],\n",
      "         ...,\n",
      "         [-5.4404e-01, -1.9802e-01,  9.2835e-02,  ..., -1.0418e+00,\n",
      "           1.1960e+00,  1.9206e+00],\n",
      "         [-5.4404e-01, -1.9802e-01,  9.2835e-02,  ..., -1.0418e+00,\n",
      "           1.1960e+00,  1.9206e+00],\n",
      "         [-5.4404e-01, -1.9802e-01,  9.2835e-02,  ..., -1.0418e+00,\n",
      "           1.1960e+00,  1.9206e+00]],\n",
      "\n",
      "        [[-8.0157e-01, -4.5159e-02, -1.3882e-01,  ..., -1.1055e+00,\n",
      "           1.8918e+00,  2.2742e+00],\n",
      "         [-8.0157e-01, -4.5159e-02, -1.3882e-01,  ..., -1.1055e+00,\n",
      "           1.8918e+00,  2.2742e+00],\n",
      "         [-8.0157e-01, -4.5159e-02, -1.3882e-01,  ..., -1.1055e+00,\n",
      "           1.8918e+00,  2.2742e+00],\n",
      "         ...,\n",
      "         [-8.0157e-01, -4.5159e-02, -1.3882e-01,  ..., -1.1055e+00,\n",
      "           1.8918e+00,  2.2742e+00],\n",
      "         [-8.0157e-01, -4.5159e-02, -1.3882e-01,  ..., -1.1055e+00,\n",
      "           1.8918e+00,  2.2742e+00],\n",
      "         [-8.0157e-01, -4.5159e-02, -1.3882e-01,  ..., -1.1055e+00,\n",
      "           1.8918e+00,  2.2742e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0304, -2.0072,  1.0362,  ..., -0.1650, -0.8491, -1.1862],\n",
      "         [-0.0304, -2.0072,  1.0362,  ..., -0.1650, -0.8491, -1.1862],\n",
      "         [-0.0304, -2.0072,  1.0362,  ..., -0.1650, -0.8491, -1.1862],\n",
      "         ...,\n",
      "         [-0.0304, -2.0072,  1.0362,  ..., -0.1650, -0.8491, -1.1862],\n",
      "         [-0.0304, -2.0072,  1.0362,  ..., -0.1650, -0.8491, -1.1862],\n",
      "         [-0.0304, -2.0072,  1.0362,  ..., -0.1650, -0.8491, -1.1862]],\n",
      "\n",
      "        [[ 0.1811, -2.3808,  1.1435,  ...,  0.5812, -0.7909, -0.8127],\n",
      "         [ 0.1811, -2.3808,  1.1435,  ...,  0.5812, -0.7909, -0.8127],\n",
      "         [ 0.1811, -2.3808,  1.1435,  ...,  0.5812, -0.7909, -0.8127],\n",
      "         ...,\n",
      "         [ 0.1811, -2.3808,  1.1435,  ...,  0.5812, -0.7909, -0.8127],\n",
      "         [ 0.1811, -2.3808,  1.1435,  ...,  0.5812, -0.7909, -0.8127],\n",
      "         [ 0.1811, -2.3808,  1.1435,  ...,  0.5812, -0.7909, -0.8127]],\n",
      "\n",
      "        [[ 0.1753, -2.2774,  1.0044,  ...,  0.8640, -1.1716, -0.8749],\n",
      "         [ 0.1753, -2.2774,  1.0044,  ...,  0.8640, -1.1716, -0.8749],\n",
      "         [ 0.1753, -2.2774,  1.0044,  ...,  0.8640, -1.1716, -0.8749],\n",
      "         ...,\n",
      "         [ 0.1753, -2.2774,  1.0044,  ...,  0.8640, -1.1716, -0.8749],\n",
      "         [ 0.1753, -2.2774,  1.0044,  ...,  0.8640, -1.1716, -0.8749],\n",
      "         [ 0.1753, -2.2774,  1.0044,  ...,  0.8640, -1.1716, -0.8749]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2218, -2.2717,  1.1276,  ...,  0.3742, -0.5219, -0.8864],\n",
      "         [ 0.2218, -2.2717,  1.1276,  ...,  0.3742, -0.5219, -0.8864],\n",
      "         [ 0.2218, -2.2717,  1.1276,  ...,  0.3742, -0.5219, -0.8864],\n",
      "         ...,\n",
      "         [ 0.2218, -2.2717,  1.1276,  ...,  0.3742, -0.5219, -0.8864],\n",
      "         [ 0.2218, -2.2717,  1.1276,  ...,  0.3742, -0.5219, -0.8864],\n",
      "         [ 0.2218, -2.2717,  1.1276,  ...,  0.3742, -0.5219, -0.8864]],\n",
      "\n",
      "        [[ 0.0088, -2.6212,  0.8103,  ...,  0.6048, -1.2324, -0.9535],\n",
      "         [ 0.0088, -2.6212,  0.8103,  ...,  0.6048, -1.2324, -0.9535],\n",
      "         [ 0.0088, -2.6212,  0.8103,  ...,  0.6048, -1.2324, -0.9535],\n",
      "         ...,\n",
      "         [ 0.0088, -2.6212,  0.8103,  ...,  0.6048, -1.2324, -0.9535],\n",
      "         [ 0.0088, -2.6212,  0.8103,  ...,  0.6048, -1.2324, -0.9535],\n",
      "         [ 0.0088, -2.6212,  0.8103,  ...,  0.6048, -1.2324, -0.9535]],\n",
      "\n",
      "        [[ 0.3115, -2.2326,  1.2575,  ...,  0.2449, -0.8081, -1.1774],\n",
      "         [ 0.3115, -2.2326,  1.2575,  ...,  0.2449, -0.8081, -1.1774],\n",
      "         [ 0.3115, -2.2326,  1.2575,  ...,  0.2449, -0.8081, -1.1774],\n",
      "         ...,\n",
      "         [ 0.3115, -2.2326,  1.2575,  ...,  0.2449, -0.8081, -1.1774],\n",
      "         [ 0.3115, -2.2326,  1.2575,  ...,  0.2449, -0.8081, -1.1774],\n",
      "         [ 0.3115, -2.2326,  1.2575,  ...,  0.2449, -0.8081, -1.1774]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2352,  0.0931,  1.6018,  ..., -0.3062, -0.8019,  0.5020],\n",
      "         [ 0.2352,  0.0931,  1.6018,  ..., -0.3062, -0.8019,  0.5020],\n",
      "         [ 0.2352,  0.0931,  1.6018,  ..., -0.3062, -0.8019,  0.5020],\n",
      "         ...,\n",
      "         [ 0.2352,  0.0931,  1.6018,  ..., -0.3062, -0.8019,  0.5020],\n",
      "         [ 0.2352,  0.0931,  1.6018,  ..., -0.3062, -0.8019,  0.5020],\n",
      "         [ 0.2352,  0.0931,  1.6018,  ..., -0.3062, -0.8019,  0.5020]],\n",
      "\n",
      "        [[-0.7801, -0.0537,  2.0757,  ..., -1.5508, -0.8779, -0.1362],\n",
      "         [-0.7801, -0.0537,  2.0757,  ..., -1.5508, -0.8779, -0.1362],\n",
      "         [-0.7801, -0.0537,  2.0757,  ..., -1.5508, -0.8779, -0.1362],\n",
      "         ...,\n",
      "         [-0.7801, -0.0537,  2.0757,  ..., -1.5508, -0.8779, -0.1362],\n",
      "         [-0.7801, -0.0537,  2.0757,  ..., -1.5508, -0.8779, -0.1362],\n",
      "         [-0.7801, -0.0537,  2.0757,  ..., -1.5508, -0.8779, -0.1362]],\n",
      "\n",
      "        [[-0.7715, -0.7365,  1.7926,  ..., -1.1183, -0.4455, -0.0900],\n",
      "         [-0.7715, -0.7365,  1.7926,  ..., -1.1183, -0.4455, -0.0900],\n",
      "         [-0.7715, -0.7365,  1.7926,  ..., -1.1183, -0.4455, -0.0900],\n",
      "         ...,\n",
      "         [-0.7715, -0.7365,  1.7926,  ..., -1.1183, -0.4455, -0.0900],\n",
      "         [-0.7715, -0.7365,  1.7926,  ..., -1.1183, -0.4455, -0.0900],\n",
      "         [-0.7715, -0.7365,  1.7926,  ..., -1.1183, -0.4455, -0.0900]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4148,  0.0353,  1.8410,  ..., -0.8758, -0.8599,  0.0291],\n",
      "         [-0.4148,  0.0353,  1.8410,  ..., -0.8758, -0.8599,  0.0291],\n",
      "         [-0.4148,  0.0353,  1.8410,  ..., -0.8758, -0.8599,  0.0291],\n",
      "         ...,\n",
      "         [-0.4148,  0.0353,  1.8410,  ..., -0.8758, -0.8599,  0.0291],\n",
      "         [-0.4148,  0.0353,  1.8410,  ..., -0.8758, -0.8599,  0.0291],\n",
      "         [-0.4148,  0.0353,  1.8410,  ..., -0.8758, -0.8599,  0.0291]],\n",
      "\n",
      "        [[-0.7243,  0.1722,  1.9607,  ..., -1.1487, -1.1095, -0.0601],\n",
      "         [-0.7243,  0.1722,  1.9607,  ..., -1.1487, -1.1095, -0.0601],\n",
      "         [-0.7243,  0.1722,  1.9607,  ..., -1.1487, -1.1095, -0.0601],\n",
      "         ...,\n",
      "         [-0.7243,  0.1722,  1.9607,  ..., -1.1487, -1.1095, -0.0601],\n",
      "         [-0.7243,  0.1722,  1.9607,  ..., -1.1487, -1.1095, -0.0601],\n",
      "         [-0.7243,  0.1722,  1.9607,  ..., -1.1487, -1.1095, -0.0601]],\n",
      "\n",
      "        [[-0.4316,  0.0362,  1.7960,  ..., -1.0222, -0.7485, -0.0416],\n",
      "         [-0.4316,  0.0362,  1.7960,  ..., -1.0222, -0.7485, -0.0416],\n",
      "         [-0.4316,  0.0362,  1.7960,  ..., -1.0222, -0.7485, -0.0416],\n",
      "         ...,\n",
      "         [-0.4316,  0.0362,  1.7960,  ..., -1.0222, -0.7485, -0.0416],\n",
      "         [-0.4316,  0.0362,  1.7960,  ..., -1.0222, -0.7485, -0.0416],\n",
      "         [-0.4316,  0.0362,  1.7960,  ..., -1.0222, -0.7485, -0.0416]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.0997, -0.2035, -0.5311,  ..., -1.6348,  0.5674, -0.6880],\n",
      "         [-1.0997, -0.2035, -0.5311,  ..., -1.6348,  0.5674, -0.6880],\n",
      "         [-1.0997, -0.2035, -0.5311,  ..., -1.6348,  0.5674, -0.6880],\n",
      "         ...,\n",
      "         [-1.0997, -0.2035, -0.5311,  ..., -1.6348,  0.5674, -0.6880],\n",
      "         [-1.0997, -0.2035, -0.5311,  ..., -1.6348,  0.5674, -0.6880],\n",
      "         [-1.0997, -0.2035, -0.5311,  ..., -1.6348,  0.5674, -0.6880]],\n",
      "\n",
      "        [[-0.5817, -0.2827, -0.1661,  ..., -1.7150,  1.2227, -0.1794],\n",
      "         [-0.5817, -0.2827, -0.1661,  ..., -1.7150,  1.2227, -0.1794],\n",
      "         [-0.5817, -0.2827, -0.1661,  ..., -1.7150,  1.2227, -0.1794],\n",
      "         ...,\n",
      "         [-0.5817, -0.2827, -0.1661,  ..., -1.7150,  1.2227, -0.1794],\n",
      "         [-0.5817, -0.2827, -0.1661,  ..., -1.7150,  1.2227, -0.1794],\n",
      "         [-0.5817, -0.2827, -0.1661,  ..., -1.7150,  1.2227, -0.1794]],\n",
      "\n",
      "        [[-0.6393,  0.0323,  0.2661,  ..., -1.3868,  1.0143, -0.1411],\n",
      "         [-0.6393,  0.0323,  0.2661,  ..., -1.3868,  1.0143, -0.1411],\n",
      "         [-0.6393,  0.0323,  0.2661,  ..., -1.3868,  1.0143, -0.1411],\n",
      "         ...,\n",
      "         [-0.6393,  0.0323,  0.2661,  ..., -1.3868,  1.0143, -0.1410],\n",
      "         [-0.6393,  0.0323,  0.2661,  ..., -1.3868,  1.0143, -0.1410],\n",
      "         [-0.6393,  0.0323,  0.2661,  ..., -1.3868,  1.0143, -0.1410]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7179, -0.1604, -0.3935,  ..., -1.7426,  1.1486, -0.3142],\n",
      "         [-0.7179, -0.1604, -0.3935,  ..., -1.7426,  1.1486, -0.3142],\n",
      "         [-0.7179, -0.1604, -0.3935,  ..., -1.7426,  1.1486, -0.3142],\n",
      "         ...,\n",
      "         [-0.7179, -0.1604, -0.3935,  ..., -1.7426,  1.1486, -0.3142],\n",
      "         [-0.7179, -0.1604, -0.3935,  ..., -1.7426,  1.1486, -0.3142],\n",
      "         [-0.7179, -0.1604, -0.3935,  ..., -1.7426,  1.1486, -0.3142]],\n",
      "\n",
      "        [[-0.9412, -0.3852, -0.4059,  ..., -1.6855,  0.9099, -0.4069],\n",
      "         [-0.9412, -0.3852, -0.4059,  ..., -1.6855,  0.9099, -0.4069],\n",
      "         [-0.9412, -0.3852, -0.4059,  ..., -1.6855,  0.9099, -0.4069],\n",
      "         ...,\n",
      "         [-0.9412, -0.3852, -0.4059,  ..., -1.6855,  0.9099, -0.4069],\n",
      "         [-0.9412, -0.3852, -0.4059,  ..., -1.6855,  0.9099, -0.4069],\n",
      "         [-0.9412, -0.3852, -0.4059,  ..., -1.6855,  0.9099, -0.4069]],\n",
      "\n",
      "        [[-0.8340, -0.0679, -0.3022,  ..., -1.6642,  1.1581, -0.3085],\n",
      "         [-0.8340, -0.0679, -0.3022,  ..., -1.6642,  1.1581, -0.3085],\n",
      "         [-0.8340, -0.0679, -0.3022,  ..., -1.6642,  1.1581, -0.3085],\n",
      "         ...,\n",
      "         [-0.8340, -0.0679, -0.3022,  ..., -1.6642,  1.1581, -0.3085],\n",
      "         [-0.8340, -0.0679, -0.3022,  ..., -1.6642,  1.1581, -0.3085],\n",
      "         [-0.8340, -0.0679, -0.3022,  ..., -1.6642,  1.1581, -0.3085]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6762,  1.2949,  0.1909,  ..., -1.3210, -1.6511,  1.4693],\n",
      "         [-0.6762,  1.2949,  0.1909,  ..., -1.3210, -1.6511,  1.4693],\n",
      "         [-0.6762,  1.2949,  0.1909,  ..., -1.3210, -1.6511,  1.4693],\n",
      "         ...,\n",
      "         [-0.6762,  1.2949,  0.1909,  ..., -1.3210, -1.6511,  1.4693],\n",
      "         [-0.6762,  1.2949,  0.1909,  ..., -1.3210, -1.6511,  1.4693],\n",
      "         [-0.6762,  1.2949,  0.1909,  ..., -1.3210, -1.6511,  1.4693]],\n",
      "\n",
      "        [[-0.9916,  1.5827,  0.5713,  ..., -0.6246, -2.0544,  1.3360],\n",
      "         [-0.9916,  1.5827,  0.5713,  ..., -0.6246, -2.0544,  1.3360],\n",
      "         [-0.9916,  1.5827,  0.5713,  ..., -0.6246, -2.0544,  1.3360],\n",
      "         ...,\n",
      "         [-0.9916,  1.5827,  0.5713,  ..., -0.6246, -2.0544,  1.3360],\n",
      "         [-0.9916,  1.5827,  0.5713,  ..., -0.6246, -2.0544,  1.3360],\n",
      "         [-0.9916,  1.5827,  0.5713,  ..., -0.6246, -2.0544,  1.3360]],\n",
      "\n",
      "        [[-0.9952,  1.3126,  0.3017,  ..., -0.4899, -1.9809,  1.1294],\n",
      "         [-0.9952,  1.3126,  0.3017,  ..., -0.4899, -1.9809,  1.1294],\n",
      "         [-0.9952,  1.3126,  0.3017,  ..., -0.4899, -1.9809,  1.1294],\n",
      "         ...,\n",
      "         [-0.9952,  1.3126,  0.3017,  ..., -0.4898, -1.9809,  1.1294],\n",
      "         [-0.9952,  1.3126,  0.3017,  ..., -0.4898, -1.9809,  1.1294],\n",
      "         [-0.9952,  1.3126,  0.3017,  ..., -0.4898, -1.9809,  1.1294]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7062,  1.4827,  0.2196,  ..., -0.9298, -1.8236,  1.3009],\n",
      "         [-0.7062,  1.4827,  0.2196,  ..., -0.9298, -1.8236,  1.3009],\n",
      "         [-0.7062,  1.4827,  0.2196,  ..., -0.9298, -1.8236,  1.3009],\n",
      "         ...,\n",
      "         [-0.7062,  1.4827,  0.2196,  ..., -0.9298, -1.8236,  1.3009],\n",
      "         [-0.7062,  1.4827,  0.2196,  ..., -0.9298, -1.8236,  1.3009],\n",
      "         [-0.7062,  1.4827,  0.2196,  ..., -0.9298, -1.8236,  1.3009]],\n",
      "\n",
      "        [[-0.8464,  1.4712,  0.4191,  ..., -0.7658, -2.1156,  1.3169],\n",
      "         [-0.8464,  1.4712,  0.4191,  ..., -0.7658, -2.1156,  1.3169],\n",
      "         [-0.8464,  1.4712,  0.4191,  ..., -0.7658, -2.1156,  1.3169],\n",
      "         ...,\n",
      "         [-0.8464,  1.4712,  0.4191,  ..., -0.7658, -2.1156,  1.3169],\n",
      "         [-0.8464,  1.4712,  0.4191,  ..., -0.7658, -2.1156,  1.3169],\n",
      "         [-0.8464,  1.4712,  0.4191,  ..., -0.7658, -2.1156,  1.3169]],\n",
      "\n",
      "        [[-0.8619,  1.4622,  0.3542,  ..., -0.7168, -1.9754,  1.2049],\n",
      "         [-0.8619,  1.4622,  0.3542,  ..., -0.7168, -1.9754,  1.2049],\n",
      "         [-0.8619,  1.4622,  0.3542,  ..., -0.7168, -1.9754,  1.2049],\n",
      "         ...,\n",
      "         [-0.8619,  1.4622,  0.3542,  ..., -0.7168, -1.9754,  1.2049],\n",
      "         [-0.8619,  1.4622,  0.3542,  ..., -0.7168, -1.9754,  1.2049],\n",
      "         [-0.8619,  1.4622,  0.3542,  ..., -0.7168, -1.9754,  1.2049]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7849,  0.0782,  0.9488,  ...,  1.0743,  0.4192,  1.0538],\n",
      "         [ 0.7849,  0.0782,  0.9488,  ...,  1.0743,  0.4192,  1.0538],\n",
      "         [ 0.7849,  0.0782,  0.9488,  ...,  1.0743,  0.4192,  1.0538],\n",
      "         ...,\n",
      "         [ 0.7849,  0.0782,  0.9488,  ...,  1.0743,  0.4192,  1.0538],\n",
      "         [ 0.7849,  0.0782,  0.9488,  ...,  1.0743,  0.4192,  1.0538],\n",
      "         [ 0.7849,  0.0782,  0.9488,  ...,  1.0743,  0.4192,  1.0538]],\n",
      "\n",
      "        [[ 0.4992,  0.3697,  0.4894,  ..., -0.0643,  0.8723,  1.2300],\n",
      "         [ 0.4992,  0.3697,  0.4894,  ..., -0.0643,  0.8723,  1.2300],\n",
      "         [ 0.4992,  0.3697,  0.4894,  ..., -0.0643,  0.8723,  1.2300],\n",
      "         ...,\n",
      "         [ 0.4992,  0.3697,  0.4894,  ..., -0.0643,  0.8723,  1.2300],\n",
      "         [ 0.4992,  0.3697,  0.4894,  ..., -0.0643,  0.8723,  1.2300],\n",
      "         [ 0.4992,  0.3697,  0.4894,  ..., -0.0643,  0.8723,  1.2300]],\n",
      "\n",
      "        [[ 0.8622,  0.4233,  1.0639,  ...,  0.6565,  0.3605,  1.5137],\n",
      "         [ 0.8622,  0.4233,  1.0639,  ...,  0.6565,  0.3605,  1.5137],\n",
      "         [ 0.8622,  0.4233,  1.0639,  ...,  0.6565,  0.3605,  1.5137],\n",
      "         ...,\n",
      "         [ 0.8622,  0.4233,  1.0639,  ...,  0.6565,  0.3605,  1.5137],\n",
      "         [ 0.8622,  0.4233,  1.0639,  ...,  0.6565,  0.3605,  1.5137],\n",
      "         [ 0.8622,  0.4233,  1.0639,  ...,  0.6565,  0.3605,  1.5137]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.7476,  0.4654,  0.9113,  ...,  0.7026,  0.7013,  1.1741],\n",
      "         [ 0.7476,  0.4654,  0.9113,  ...,  0.7026,  0.7013,  1.1741],\n",
      "         [ 0.7476,  0.4654,  0.9113,  ...,  0.7026,  0.7013,  1.1741],\n",
      "         ...,\n",
      "         [ 0.7476,  0.4654,  0.9113,  ...,  0.7026,  0.7013,  1.1741],\n",
      "         [ 0.7476,  0.4654,  0.9113,  ...,  0.7026,  0.7013,  1.1741],\n",
      "         [ 0.7476,  0.4654,  0.9113,  ...,  0.7026,  0.7013,  1.1741]],\n",
      "\n",
      "        [[ 0.8107,  0.2938,  0.5801,  ...,  0.1315,  0.5978,  1.1018],\n",
      "         [ 0.8107,  0.2938,  0.5801,  ...,  0.1315,  0.5978,  1.1018],\n",
      "         [ 0.8107,  0.2938,  0.5801,  ...,  0.1315,  0.5978,  1.1018],\n",
      "         ...,\n",
      "         [ 0.8107,  0.2938,  0.5801,  ...,  0.1315,  0.5978,  1.1018],\n",
      "         [ 0.8107,  0.2938,  0.5801,  ...,  0.1315,  0.5978,  1.1018],\n",
      "         [ 0.8107,  0.2938,  0.5801,  ...,  0.1315,  0.5978,  1.1018]],\n",
      "\n",
      "        [[ 0.8313,  0.4866,  0.7555,  ...,  0.5529,  0.6013,  1.3287],\n",
      "         [ 0.8313,  0.4866,  0.7555,  ...,  0.5529,  0.6013,  1.3287],\n",
      "         [ 0.8313,  0.4866,  0.7555,  ...,  0.5529,  0.6013,  1.3287],\n",
      "         ...,\n",
      "         [ 0.8313,  0.4866,  0.7555,  ...,  0.5529,  0.6013,  1.3287],\n",
      "         [ 0.8313,  0.4866,  0.7555,  ...,  0.5529,  0.6013,  1.3287],\n",
      "         [ 0.8313,  0.4866,  0.7555,  ...,  0.5529,  0.6013,  1.3287]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.8659, -0.6356, -0.2370,  ...,  0.5589,  0.2617, -0.9208],\n",
      "         [-1.8659, -0.6356, -0.2370,  ...,  0.5588,  0.2617, -0.9208],\n",
      "         [-1.8659, -0.6356, -0.2370,  ...,  0.5589,  0.2617, -0.9208],\n",
      "         ...,\n",
      "         [-1.8659, -0.6356, -0.2370,  ...,  0.5588,  0.2617, -0.9208],\n",
      "         [-1.8659, -0.6356, -0.2370,  ...,  0.5588,  0.2617, -0.9208],\n",
      "         [-1.8659, -0.6356, -0.2370,  ...,  0.5588,  0.2617, -0.9208]],\n",
      "\n",
      "        [[-1.7036, -1.0620, -0.3894,  ...,  0.0166,  0.2913, -1.0925],\n",
      "         [-1.7036, -1.0620, -0.3894,  ...,  0.0166,  0.2913, -1.0925],\n",
      "         [-1.7036, -1.0620, -0.3894,  ...,  0.0166,  0.2913, -1.0925],\n",
      "         ...,\n",
      "         [-1.7036, -1.0620, -0.3894,  ...,  0.0166,  0.2913, -1.0925],\n",
      "         [-1.7036, -1.0620, -0.3894,  ...,  0.0166,  0.2913, -1.0925],\n",
      "         [-1.7036, -1.0620, -0.3894,  ...,  0.0166,  0.2913, -1.0925]],\n",
      "\n",
      "        [[-1.8646, -0.8884, -0.2362,  ...,  0.2405, -0.1405, -1.1592],\n",
      "         [-1.8646, -0.8884, -0.2362,  ...,  0.2405, -0.1405, -1.1592],\n",
      "         [-1.8646, -0.8884, -0.2362,  ...,  0.2405, -0.1405, -1.1592],\n",
      "         ...,\n",
      "         [-1.8646, -0.8884, -0.2362,  ...,  0.2405, -0.1405, -1.1592],\n",
      "         [-1.8646, -0.8884, -0.2362,  ...,  0.2405, -0.1405, -1.1592],\n",
      "         [-1.8646, -0.8884, -0.2362,  ...,  0.2405, -0.1405, -1.1592]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.8150, -0.8985, -0.1731,  ...,  0.2394,  0.4227, -1.0141],\n",
      "         [-1.8150, -0.8985, -0.1731,  ...,  0.2394,  0.4227, -1.0141],\n",
      "         [-1.8150, -0.8985, -0.1731,  ...,  0.2394,  0.4227, -1.0141],\n",
      "         ...,\n",
      "         [-1.8150, -0.8985, -0.1731,  ...,  0.2394,  0.4227, -1.0141],\n",
      "         [-1.8150, -0.8985, -0.1731,  ...,  0.2394,  0.4227, -1.0141],\n",
      "         [-1.8150, -0.8985, -0.1731,  ...,  0.2394,  0.4227, -1.0141]],\n",
      "\n",
      "        [[-1.5166, -0.7245, -0.1970,  ...,  0.2555, -0.1324, -1.1854],\n",
      "         [-1.5166, -0.7245, -0.1970,  ...,  0.2555, -0.1324, -1.1854],\n",
      "         [-1.5166, -0.7245, -0.1970,  ...,  0.2555, -0.1324, -1.1854],\n",
      "         ...,\n",
      "         [-1.5166, -0.7245, -0.1970,  ...,  0.2555, -0.1324, -1.1854],\n",
      "         [-1.5166, -0.7245, -0.1970,  ...,  0.2555, -0.1324, -1.1854],\n",
      "         [-1.5166, -0.7245, -0.1970,  ...,  0.2555, -0.1324, -1.1854]],\n",
      "\n",
      "        [[-1.8367, -0.9180,  0.0060,  ...,  0.3751,  0.3054, -1.0516],\n",
      "         [-1.8367, -0.9180,  0.0060,  ...,  0.3751,  0.3054, -1.0516],\n",
      "         [-1.8367, -0.9180,  0.0060,  ...,  0.3751,  0.3054, -1.0516],\n",
      "         ...,\n",
      "         [-1.8367, -0.9180,  0.0060,  ...,  0.3751,  0.3054, -1.0516],\n",
      "         [-1.8367, -0.9180,  0.0060,  ...,  0.3751,  0.3054, -1.0516],\n",
      "         [-1.8367, -0.9180,  0.0060,  ...,  0.3751,  0.3054, -1.0516]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.1856,  0.6804, -1.1950,  ..., -0.8471, -0.6713,  0.3581],\n",
      "         [-1.1856,  0.6804, -1.1950,  ..., -0.8471, -0.6713,  0.3581],\n",
      "         [-1.1856,  0.6804, -1.1950,  ..., -0.8471, -0.6713,  0.3581],\n",
      "         ...,\n",
      "         [-1.1856,  0.6804, -1.1950,  ..., -0.8471, -0.6713,  0.3581],\n",
      "         [-1.1856,  0.6804, -1.1950,  ..., -0.8471, -0.6713,  0.3581],\n",
      "         [-1.1856,  0.6804, -1.1950,  ..., -0.8471, -0.6713,  0.3581]],\n",
      "\n",
      "        [[-0.7889,  0.5453, -0.8650,  ..., -0.6767, -1.2967,  0.2572],\n",
      "         [-0.7889,  0.5453, -0.8650,  ..., -0.6767, -1.2967,  0.2572],\n",
      "         [-0.7889,  0.5453, -0.8650,  ..., -0.6767, -1.2967,  0.2572],\n",
      "         ...,\n",
      "         [-0.7889,  0.5453, -0.8650,  ..., -0.6767, -1.2967,  0.2572],\n",
      "         [-0.7889,  0.5453, -0.8650,  ..., -0.6767, -1.2967,  0.2572],\n",
      "         [-0.7889,  0.5453, -0.8650,  ..., -0.6767, -1.2967,  0.2572]],\n",
      "\n",
      "        [[-0.8629,  0.3813, -0.5376,  ..., -0.4960, -1.1678,  0.1535],\n",
      "         [-0.8629,  0.3813, -0.5376,  ..., -0.4960, -1.1678,  0.1535],\n",
      "         [-0.8629,  0.3813, -0.5376,  ..., -0.4960, -1.1678,  0.1535],\n",
      "         ...,\n",
      "         [-0.8629,  0.3813, -0.5376,  ..., -0.4960, -1.1678,  0.1535],\n",
      "         [-0.8629,  0.3813, -0.5376,  ..., -0.4960, -1.1678,  0.1535],\n",
      "         [-0.8629,  0.3813, -0.5376,  ..., -0.4960, -1.1678,  0.1535]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8896,  0.7692, -0.9006,  ..., -0.5690, -1.2182,  0.5267],\n",
      "         [-0.8896,  0.7692, -0.9006,  ..., -0.5690, -1.2182,  0.5267],\n",
      "         [-0.8896,  0.7692, -0.9006,  ..., -0.5690, -1.2182,  0.5267],\n",
      "         ...,\n",
      "         [-0.8896,  0.7692, -0.9006,  ..., -0.5690, -1.2182,  0.5267],\n",
      "         [-0.8896,  0.7692, -0.9006,  ..., -0.5690, -1.2182,  0.5267],\n",
      "         [-0.8896,  0.7692, -0.9006,  ..., -0.5690, -1.2182,  0.5267]],\n",
      "\n",
      "        [[-0.6895,  0.5240, -0.8990,  ..., -0.6910, -1.3692,  0.1665],\n",
      "         [-0.6895,  0.5240, -0.8990,  ..., -0.6910, -1.3692,  0.1665],\n",
      "         [-0.6895,  0.5240, -0.8990,  ..., -0.6910, -1.3692,  0.1665],\n",
      "         ...,\n",
      "         [-0.6895,  0.5240, -0.8990,  ..., -0.6910, -1.3692,  0.1665],\n",
      "         [-0.6895,  0.5240, -0.8990,  ..., -0.6910, -1.3692,  0.1665],\n",
      "         [-0.6895,  0.5240, -0.8990,  ..., -0.6910, -1.3692,  0.1665]],\n",
      "\n",
      "        [[-0.8361,  0.7608, -0.8174,  ..., -0.7684, -1.4118,  0.4141],\n",
      "         [-0.8361,  0.7608, -0.8174,  ..., -0.7684, -1.4118,  0.4141],\n",
      "         [-0.8361,  0.7608, -0.8174,  ..., -0.7684, -1.4118,  0.4141],\n",
      "         ...,\n",
      "         [-0.8361,  0.7608, -0.8174,  ..., -0.7684, -1.4118,  0.4141],\n",
      "         [-0.8361,  0.7608, -0.8174,  ..., -0.7684, -1.4118,  0.4141],\n",
      "         [-0.8361,  0.7608, -0.8174,  ..., -0.7684, -1.4118,  0.4141]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5745, -0.5172, -0.9554,  ..., -1.3763, -0.3498, -0.9079],\n",
      "         [ 0.5745, -0.5172, -0.9554,  ..., -1.3763, -0.3498, -0.9079],\n",
      "         [ 0.5745, -0.5172, -0.9554,  ..., -1.3763, -0.3498, -0.9079],\n",
      "         ...,\n",
      "         [ 0.5745, -0.5172, -0.9554,  ..., -1.3763, -0.3498, -0.9079],\n",
      "         [ 0.5745, -0.5172, -0.9554,  ..., -1.3763, -0.3498, -0.9079],\n",
      "         [ 0.5745, -0.5172, -0.9554,  ..., -1.3763, -0.3498, -0.9079]],\n",
      "\n",
      "        [[ 0.7325, -1.1081,  0.1520,  ..., -1.9105,  0.7454, -1.7048],\n",
      "         [ 0.7325, -1.1081,  0.1520,  ..., -1.9105,  0.7454, -1.7048],\n",
      "         [ 0.7325, -1.1081,  0.1520,  ..., -1.9105,  0.7454, -1.7048],\n",
      "         ...,\n",
      "         [ 0.7325, -1.1081,  0.1520,  ..., -1.9105,  0.7454, -1.7048],\n",
      "         [ 0.7325, -1.1081,  0.1520,  ..., -1.9105,  0.7454, -1.7048],\n",
      "         [ 0.7325, -1.1081,  0.1520,  ..., -1.9105,  0.7454, -1.7048]],\n",
      "\n",
      "        [[ 1.0690, -0.8603,  0.2912,  ..., -1.9342,  0.7223, -1.5622],\n",
      "         [ 1.0690, -0.8603,  0.2912,  ..., -1.9342,  0.7223, -1.5622],\n",
      "         [ 1.0690, -0.8603,  0.2912,  ..., -1.9342,  0.7223, -1.5622],\n",
      "         ...,\n",
      "         [ 1.0690, -0.8603,  0.2912,  ..., -1.9342,  0.7223, -1.5622],\n",
      "         [ 1.0690, -0.8603,  0.2912,  ..., -1.9342,  0.7223, -1.5622],\n",
      "         [ 1.0690, -0.8603,  0.2912,  ..., -1.9342,  0.7223, -1.5622]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.6939, -0.7704, -0.1324,  ..., -1.6373,  0.0737, -1.4013],\n",
      "         [ 0.6939, -0.7704, -0.1324,  ..., -1.6373,  0.0737, -1.4013],\n",
      "         [ 0.6939, -0.7704, -0.1324,  ..., -1.6373,  0.0737, -1.4013],\n",
      "         ...,\n",
      "         [ 0.6939, -0.7704, -0.1324,  ..., -1.6373,  0.0737, -1.4013],\n",
      "         [ 0.6939, -0.7704, -0.1324,  ..., -1.6373,  0.0737, -1.4013],\n",
      "         [ 0.6939, -0.7704, -0.1324,  ..., -1.6373,  0.0737, -1.4013]],\n",
      "\n",
      "        [[ 0.8818, -1.0561, -0.0391,  ..., -1.7381,  0.6232, -1.5943],\n",
      "         [ 0.8818, -1.0561, -0.0391,  ..., -1.7381,  0.6232, -1.5943],\n",
      "         [ 0.8818, -1.0561, -0.0391,  ..., -1.7381,  0.6232, -1.5943],\n",
      "         ...,\n",
      "         [ 0.8818, -1.0561, -0.0391,  ..., -1.7381,  0.6232, -1.5943],\n",
      "         [ 0.8818, -1.0561, -0.0391,  ..., -1.7381,  0.6232, -1.5943],\n",
      "         [ 0.8818, -1.0561, -0.0391,  ..., -1.7381,  0.6232, -1.5943]],\n",
      "\n",
      "        [[ 0.5914, -0.9040, -0.1283,  ..., -1.6529,  0.1503, -1.2650],\n",
      "         [ 0.5914, -0.9040, -0.1283,  ..., -1.6529,  0.1503, -1.2650],\n",
      "         [ 0.5914, -0.9040, -0.1283,  ..., -1.6529,  0.1503, -1.2650],\n",
      "         ...,\n",
      "         [ 0.5914, -0.9040, -0.1283,  ..., -1.6529,  0.1503, -1.2650],\n",
      "         [ 0.5914, -0.9040, -0.1283,  ..., -1.6529,  0.1503, -1.2650],\n",
      "         [ 0.5914, -0.9040, -0.1283,  ..., -1.6529,  0.1503, -1.2650]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1606, -0.1476,  1.8191,  ...,  1.0228,  0.1623, -0.6373],\n",
      "         [ 0.1606, -0.1476,  1.8191,  ...,  1.0228,  0.1623, -0.6373],\n",
      "         [ 0.1606, -0.1476,  1.8191,  ...,  1.0228,  0.1623, -0.6373],\n",
      "         ...,\n",
      "         [ 0.1606, -0.1476,  1.8191,  ...,  1.0228,  0.1623, -0.6373],\n",
      "         [ 0.1606, -0.1476,  1.8191,  ...,  1.0228,  0.1623, -0.6373],\n",
      "         [ 0.1606, -0.1476,  1.8191,  ...,  1.0228,  0.1623, -0.6373]],\n",
      "\n",
      "        [[ 0.2868,  0.0740,  1.8662,  ...,  0.8014,  0.0377,  0.0068],\n",
      "         [ 0.2868,  0.0740,  1.8662,  ...,  0.8014,  0.0377,  0.0068],\n",
      "         [ 0.2868,  0.0740,  1.8662,  ...,  0.8014,  0.0377,  0.0068],\n",
      "         ...,\n",
      "         [ 0.2868,  0.0740,  1.8662,  ...,  0.8014,  0.0377,  0.0068],\n",
      "         [ 0.2868,  0.0740,  1.8662,  ...,  0.8014,  0.0377,  0.0068],\n",
      "         [ 0.2868,  0.0740,  1.8662,  ...,  0.8014,  0.0377,  0.0068]],\n",
      "\n",
      "        [[-0.0138, -0.0633,  1.6903,  ...,  0.7936, -0.3192, -0.4146],\n",
      "         [-0.0138, -0.0633,  1.6903,  ...,  0.7936, -0.3192, -0.4146],\n",
      "         [-0.0138, -0.0633,  1.6903,  ...,  0.7936, -0.3192, -0.4146],\n",
      "         ...,\n",
      "         [-0.0138, -0.0633,  1.6903,  ...,  0.7936, -0.3192, -0.4146],\n",
      "         [-0.0138, -0.0633,  1.6903,  ...,  0.7936, -0.3192, -0.4146],\n",
      "         [-0.0138, -0.0633,  1.6903,  ...,  0.7936, -0.3192, -0.4146]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2633,  0.0030,  2.0494,  ...,  1.0362,  0.1234, -0.1734],\n",
      "         [ 0.2633,  0.0030,  2.0494,  ...,  1.0362,  0.1234, -0.1734],\n",
      "         [ 0.2633,  0.0030,  2.0494,  ...,  1.0362,  0.1234, -0.1734],\n",
      "         ...,\n",
      "         [ 0.2633,  0.0030,  2.0494,  ...,  1.0362,  0.1234, -0.1734],\n",
      "         [ 0.2633,  0.0030,  2.0494,  ...,  1.0362,  0.1234, -0.1734],\n",
      "         [ 0.2633,  0.0030,  2.0494,  ...,  1.0362,  0.1234, -0.1734]],\n",
      "\n",
      "        [[ 0.5073,  0.1091,  2.1528,  ...,  0.8116,  0.2832, -0.3630],\n",
      "         [ 0.5073,  0.1091,  2.1528,  ...,  0.8116,  0.2832, -0.3630],\n",
      "         [ 0.5073,  0.1091,  2.1528,  ...,  0.8116,  0.2832, -0.3630],\n",
      "         ...,\n",
      "         [ 0.5073,  0.1091,  2.1528,  ...,  0.8116,  0.2832, -0.3630],\n",
      "         [ 0.5073,  0.1091,  2.1528,  ...,  0.8116,  0.2832, -0.3630],\n",
      "         [ 0.5073,  0.1091,  2.1528,  ...,  0.8116,  0.2832, -0.3630]],\n",
      "\n",
      "        [[ 0.2956, -0.0484,  2.1895,  ...,  0.7705,  0.0618, -0.3422],\n",
      "         [ 0.2956, -0.0484,  2.1895,  ...,  0.7705,  0.0618, -0.3422],\n",
      "         [ 0.2956, -0.0484,  2.1895,  ...,  0.7705,  0.0618, -0.3422],\n",
      "         ...,\n",
      "         [ 0.2956, -0.0484,  2.1895,  ...,  0.7705,  0.0618, -0.3422],\n",
      "         [ 0.2956, -0.0484,  2.1895,  ...,  0.7705,  0.0618, -0.3422],\n",
      "         [ 0.2956, -0.0484,  2.1895,  ...,  0.7705,  0.0618, -0.3422]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)], tensor([[-0.4703,  0.6978,  0.4900,  ..., -0.8085, -0.6941,  0.1449],\n",
      "        [-0.3609,  0.2294,  0.2298,  ..., -0.7494, -0.7294, -0.0088],\n",
      "        [-0.3849,  0.5149,  0.3007,  ..., -0.6698, -0.7672,  0.0627],\n",
      "        ...,\n",
      "        [-0.3731,  0.5069,  0.4620,  ..., -0.7555, -0.7432,  0.0652],\n",
      "        [-0.2714,  0.4904,  0.4599,  ..., -0.7487, -0.7251,  0.0119],\n",
      "        [-0.3024,  0.5257,  0.4456,  ..., -0.7785, -0.7206,  0.0551]],\n",
      "       grad_fn=<TanhBackward0>), tensor([[ 0.3423, -0.0408, -0.1215,  ...,  0.1758, -0.1429, -0.0975],\n",
      "        [-0.0531, -0.1021, -0.0730,  ...,  0.9357, -0.1529, -0.0776],\n",
      "        [-0.0917, -0.1071, -0.1296,  ...,  0.7458, -0.1074, -0.0829],\n",
      "        ...,\n",
      "        [-0.0088, -0.0567, -0.0502,  ...,  0.3875, -0.1367, -0.1061],\n",
      "        [ 0.1312, -0.0887, -0.1200,  ...,  0.5919, -0.1510, -0.0584],\n",
      "        [-0.0406, -0.0694, -0.0069,  ...,  0.4911, -0.1473, -0.0880]],\n",
      "       grad_fn=<GeluBackward0>))\n"
     ]
    }
   ],
   "source": [
    "model = ViltModel(config)\n",
    "output = model(**data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPの導入\n",
    "\n",
    "- dataloaderのprocessor部分を画像のみclip processorに変更\n",
    "- visual_embeddingをCLIPVisionModelに変更\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())\n",
    "# last_hidden_state = outputs.last_hidden_state\n",
    "# pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "# processorにclip導入\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from vilt_config import ViltConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from transformers import ViltProcessor\n",
    "from transformers import CLIPProcessor, CLIPVisionModel\n",
    "\n",
    "def id_from_filename(filename: str) ->Optional[int]:\n",
    "    match = filename_re.fullmatch(filename)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return int(match.group(1))\n",
    "\n",
    "filename_re = re.compile(r\".*(\\d{12})\\.((jpg)|(png))\")\n",
    "img_root = 'Dataset/val2014'\n",
    "file_names = [f for f in listdir(img_root) if isfile(join(img_root, f))]\n",
    "filename_to_id = {img_root + \"/\" + file: id_from_filename(file) for file in file_names}\n",
    "id_to_filename = {v:k for k,v in filename_to_id.items()}\n",
    "\n",
    "\n",
    "\n",
    "def read_data( path_question, path_annotation,config):\n",
    "    # read quention file\n",
    "    q = open(path_question)\n",
    "    f_quenstion = json.load(q)\n",
    "    questions = f_quenstion['questions'] \n",
    "    # read annotation file\n",
    "    a = open(path_annotation)\n",
    "    f_annotation = json.load(a)\n",
    "    print(f_annotation.keys())\n",
    "    annotations = f_annotation['annotations']\n",
    "    # preprocess for annotation\n",
    "    for annotation in tqdm(annotations):\n",
    "        answers = annotation['answers']\n",
    "        answer_count = {}\n",
    "        for answer in answers:\n",
    "            answer_ = answer[\"answer\"]\n",
    "            answer_count[answer_] = answer_count.get(answer_, 0) + 1\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for answer in answer_count:\n",
    "            if answer not in list(config.label2id.keys()):\n",
    "                continue\n",
    "            labels.append(config.label2id[answer])\n",
    "            score = get_score(answer_count[answer])\n",
    "            scores.append(score)\n",
    "        annotation['labels'] = labels\n",
    "        annotation['scores'] = scores\n",
    "    return questions, annotations\n",
    "        \n",
    "def get_score(count: int) -> float:\n",
    "    return min(1.0, count / 3)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    processor =  ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # create padded pixel values and corresponding pixel mask\n",
    "    encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "    \n",
    "    # create new batch\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack(input_ids)\n",
    "    batch['attention_mask'] = torch.stack(attention_mask)\n",
    "    batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "    batch['pixel_values'] = encoding['pixel_values']\n",
    "    batch['pixel_mask'] = encoding['pixel_mask']\n",
    "    batch['labels'] = torch.stack(labels)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, questions, annotations,config):\n",
    "        self.questions = questions\n",
    "        self.annotations = annotations\n",
    "        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.len = len(config.id2label)\n",
    "        self.vision_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", hidden_size = 768)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            # get image + text\n",
    "            annotation = self.annotations[idx]\n",
    "            questions = self.questions[idx]\n",
    "            image = Image.open(id_to_filename[annotation['image_id']])\n",
    "            \n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "           \n",
    "            text = questions['question']\n",
    "            \n",
    "            encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            clip_embedding = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "            encoding['pixel_values'] = clip_embedding[\"pixel_values\"]\n",
    "            #clipのprocessorのreturnはpixel_valueしかないのでここのみ変更\n",
    "            # remove batch dimension\n",
    "            for k,v in encoding.items():\n",
    "                encoding[k] = v.squeeze()\n",
    "            # add labels\n",
    "            labels = annotation['labels']\n",
    "            scores = annotation['scores']\n",
    "        \n",
    "            targets = torch.zeros(self.len)\n",
    "            for label, score in zip(labels, scores):\n",
    "                targets[label] = score\n",
    "            encoding[\"labels\"] = targets\n",
    "\n",
    "            return encoding\n",
    "\n",
    "\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     config = ViltConfig()\n",
    "#     path1 = 'Dataset/questions/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "#     path2 = 'Dataset/annotations/v2_mscoco_val2014_annotations.json'\n",
    "#     questions, annotations = read_data(path1,path2,config)\n",
    "#     processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "#     #dataset = VQADataset( questions, annotations,processor,config)\n",
    "#     dataset = VQADataset( questions, annotations,config)\n",
    "#     print(dataset[0]['pixel_values'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa38bdac91f012d3aa63752192202c1c4a31351885bf71923469150acc0ed662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
