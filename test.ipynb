{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "#from transformer_block import MultiHeadAttention, FFN, SubLayerConnection\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class ViltModel(nn.Module):\n",
    "    def __init__(self, config: ViltConfig):\n",
    "        \"\"\"\"config: 'ViltConfig' instance \"\"\"\n",
    "        super(ViltModel, self).__init__()\n",
    "        self.embeddings = ViltEmbeddings(config)\n",
    "        self.encoder = ViltEncoder(config)\n",
    "        self.pooler = ViltPooler(config)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.classifier = ViltClassifer(config)\n",
    "\n",
    "\n",
    "    def get_extended_attention_mask(self, attention_mask, input_shape, device):\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            # if config.is_decoder:\n",
    "            #     batch_size, seq_length = input_shape\n",
    "            #     seq_ids = torch.arange(seq_length, device=device)\n",
    "            #     causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "            #     # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "            #     # causal and attention masks must have same type with pytorch version < 1.3\n",
    "            #     causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "\n",
    "            #     if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "            #         prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "            #         causal_mask = torch.cat(\n",
    "            #             [\n",
    "            #                 torch.ones(\n",
    "            #                     (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
    "            #                 ),\n",
    "            #                 causal_mask,\n",
    "            #             ],\n",
    "            #             axis=-1,\n",
    "            #         )\n",
    "\n",
    "            #     extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            # else:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        #extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask, image_token_type_idx=1):\n",
    "        input_shape = input_ids.size()\n",
    "        # get text info\n",
    "        text_batch_size, seq_length = input_shape\n",
    "        device = input_ids.device\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((text_batch_size, seq_length)), device=device)\n",
    "        # get image info\n",
    "        image_batch_size =  pixel_values.shape[0] \n",
    "        if pixel_values is None:\n",
    "            pixel_values = torch.ones((image_batch_size, self.config.image_size, self.config_image_size), device=device)\n",
    "        \n",
    "        # calculate embeddings\n",
    "        embeddings, masks = self.embeddings(\n",
    "            input_ids, attention_mask, token_type_ids,\n",
    "            pixel_values, pixel_mask,image_token_type_idx )\n",
    "        \n",
    "        # input embeddings into encoder\n",
    "        # extended_attention_mask = ModuleUtilsMixin.get_extended_attention_mask(attention_mask, input_shape)\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(masks, input_shape, device)\n",
    "        encoder_output = self.encoder(embeddings, extended_attention_mask)\n",
    "        sequence_output = encoder_output[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        #classifier\n",
    "        output = self.classifier(pooled_output)\n",
    "        return encoder_output, pooled_output, output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ViltClassifer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(config.hidden_size, config.hidden_size*2)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size*2)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.fc(x)\n",
    "        output = self.norm(output)\n",
    "        output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self,config) :\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        layer = ViltLayer(config)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers\n",
    "\n",
    "\n",
    "\n",
    "class ViltLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.transformer_block = FFN(config)\n",
    "        self.shortcut = SubLayerConnection(config)\n",
    "    def forward(self,hidden_states,attention_mask):\n",
    "        attention_output = self.attention(hidden_states, hidden_states, hidden_states,\n",
    "            config.num_attention_heads, config.hidden_size, attention_mask)\n",
    "        FFN_output = self.transformer_block(attention_output)\n",
    "        layer_output = self.shortcut(FFN_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "class ViltPooler(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"taking the hidden state corresponding to the first token.\"\"\"\n",
    "        first_token_tensor = hidden_states[:,0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class ViltEmbeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        #super(ViltEmbeddings).__init__()\n",
    "        super().__init__()\n",
    "         # text embeddings\n",
    "        self.text_embeddings = BERTEmbeddings(config)\n",
    "        # patch embeddings\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        #image_size = 384, patch_size = 32\n",
    "        \n",
    "        # num_patches = config.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "        # modality type embedding\n",
    "        self.token_type_embeddings = nn.Embedding(config.modality_type_vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.config = config\n",
    "\n",
    "    def visual_embed(self,pixel_values, pixel_mask, max_image_length=200):\n",
    "        _, _, ph, pw = self.patch_embeddings.projection.weight.shape\n",
    "\n",
    "        x = self.patch_embeddings(pixel_values)\n",
    "        x_mask = pixel_mask[:, None, :, :].float()\n",
    "        x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "        x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "        x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        patch_dim = self.config.image_size // self.config.patch_size\n",
    "        spatial_pos = self.position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "        pos_embed = torch.cat(\n",
    "            [\n",
    "                nn.functional.pad(\n",
    "                    nn.functional.interpolate(\n",
    "                        spatial_pos,\n",
    "                        size=(h, w),\n",
    "                        mode=\"bilinear\",\n",
    "                        align_corners=True,\n",
    "                    ),\n",
    "                    (0, width - w, 0, height - h),\n",
    "                )\n",
    "                for h, w in zip(x_h, x_w)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "         # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "        patch_index = torch.stack(\n",
    "            torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "        ).to(device=x_mask.device)\n",
    "        patch_index = patch_index[None, None, :, :, :]\n",
    "        patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "        patch_index = patch_index.flatten(1, 3)\n",
    "        x_mask = x_mask.flatten(1)\n",
    "\n",
    "        if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "            # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "            # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "            # if self.patch_size = 32, 25 * 41 = 1025\n",
    "            # if res is 384 x 640, 12 * 20 = 240\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = effective_resolution.max()\n",
    "        else:\n",
    "            effective_resolution = x_h * x_w\n",
    "            max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "        valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "        non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "        unique_rows = valid_idx[:, 0].unique()\n",
    "        valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "        valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "        non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "        pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "        select = list()\n",
    "        for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "            if p <= 0:\n",
    "                valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "                select.append(valid_row_idx[i][valid_choice])\n",
    "            else:\n",
    "                pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "                select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "        select = torch.cat(select, dim=0)\n",
    "        x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "        x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "        # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "        patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "        pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        pos_embed = torch.cat(\n",
    "            (self.position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "        )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "        return x, x_mask, (patch_index, (height, width))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,input_ids, attention_mask, token_type_ids,\n",
    "                    pixel_values, pixel_mask,image_token_type_idx=1):\n",
    "            # 1. text embeddings\n",
    "            text_embeds = self.text_embeddings(\n",
    "                input_ids = input_ids, token_type_ids= token_type_ids )\n",
    "\n",
    "            # 2. patch embeddings\n",
    "            \"\"\"if use clip, change code here\n",
    "            for example: \n",
    "                import clip\n",
    "                model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "                image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "                image_embeds = model.encode_image(image)\n",
    "            \"\"\"\n",
    "            image_embeds, image_masks, patch_index = self.visual_embed(\n",
    "                pixel_values , pixel_mask, max_image_length=self.config.max_image_length )\n",
    "            \n",
    "            # 3. add modality type embedding\n",
    "            # text_embeds = text_embeds + self.token_type_embeddings(\n",
    "            #     torch.zeros_like(attention_mask,dtype=torch.long, device = text_embeds.device))\n",
    "\n",
    "            image_embeds = image_embeds + self.token_type_embeddings(\n",
    "                torch.full_like(image_masks, image_token_type_idx, dtype=torch.long, device=image_embeds.device))\n",
    "\n",
    "            # 4. concat\n",
    "            embeddings = torch.cat([text_embeds, image_embeds], dim =1)\n",
    "            masks = torch.cat([attention_mask, image_masks], dim=1)\n",
    "            return embeddings, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from vilt_config import ViltConfig\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "            super(Attention, self).__init__()\n",
    "            \n",
    "    def forward(self, query, key, value, attention_mask,attention_dropout_prob):\n",
    "        scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(query.size(-1)) #(query * key^T) / √d_k\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "            #0のところを∞に置き換えてマスクする？\n",
    "\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        #行方向にsoftmax\n",
    "\n",
    "        if attention_dropout_prob is not None:\n",
    "            attn = attention_dropout_prob(scores)\n",
    "        \n",
    "        # attn: [batch_size, head, 1, 1], value: [batch_size, head, 1, d_k]\n",
    "        # attn*value = [batch_size, head, 1, d_k]\n",
    "        return torch.matmul(attn, value), attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        #super(MultiHeadAttention).__init__()\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(config.hidden_size, config.hidden_size) for _ in range(config.num_attention_heads)])\n",
    "        self.output_linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attention = Attention(config)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,query, key, value, head, hidden_size, attention_mask=None, attention_dropout_prob=None):\n",
    "        batch_size = query.size(0)\n",
    "        d_k = hidden_size // head\n",
    "\n",
    "        query, key, value = [l(x).view(batch_size, -1, head, d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "                            #h個のattention層に分けるために(batch_size, -1, head, d_k)の形にする\n",
    "        \n",
    "        attention_output, attn = self.attention(query, key, value, attention_mask, attention_dropout_prob)\n",
    "        \n",
    "        attention_output = attention_output.transpose(1,2).contiguous().view(batch_size, -1, head * d_k)\n",
    "        #41行目でtranspose(1,2)してたので戻すためのtranspose\n",
    "        #viewを使うときは要素順に並んでいないといけないのでそのためのcontiguous()\n",
    "        #[batch_size, 1, hidden_size] で出力\n",
    "\n",
    "\n",
    "        return self.output_linear(attention_output)\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(FFN,self).__init__()\n",
    "        self.dense  = nn.Linear(config.hidden_size,config.intermediate_size)\n",
    "        self.activation = nn.GELU()\n",
    "        #configから文字じゃなくてメソッドを呼び起こす方法がわからない\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "class SubLayerConnection(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(SubLayerConnection,self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,hidden_states,input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.norm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e41b45951e4a918632e0481f6442a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from text_preprocess import BERTEmbeddings\n",
    "from img_preprocess import PatchEmbeddings\n",
    "from vilt_config import ViltConfig\n",
    "import copy\n",
    "from transformers.modeling_utils import  ModuleUtilsMixin\n",
    "from transformer_block import MultiHeadAttention\n",
    "\n",
    "from transformers import ViltProcessor\n",
    "from dataset import VQADataset, read_data, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "config = ViltConfig()\n",
    "# model = ViltModel(config)\n",
    "path1 = 'Dataset/questions/v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "path2 = 'Dataset/annotations/v2_mscoco_val2014_annotations.json'\n",
    "questions, annotations = read_data(path1, path2, config)\n",
    "# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "dataset = VQADataset(questions, annotations, config)\n",
    "dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)\n",
    "# for datas in tqdm(dataloader):\n",
    "#     data = {k: v for k, v in datas.items()}\n",
    "#     data.pop('labels')\n",
    "#     print(data.keys())\n",
    "# data\n",
    "data = next(iter(dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a36bc6f25e84569912842cc531aa3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values', 'pixel_mask'])\n"
     ]
    }
   ],
   "source": [
    "for datas in tqdm(dataloader):\n",
    "    data = {k: v for k, v in datas.items()}\n",
    "    data.pop('labels')\n",
    "    print(data.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 40])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['token_type_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_output: torch.Size([32, 269, 768])\n"
     ]
    }
   ],
   "source": [
    "model = ViltModel(config)\n",
    "output = model(**data)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViltModel input_shape: torch.Size([32, 40])\n",
      "ViltEmbedding text_embeds: torch.Size([32, 40, 768])\n",
      "ViltEmbedding image_embeds: torch.Size([32, 229, 768])\n",
      "ViltModel enbeddings: torch.Size([32, 269, 768])\n",
      "masks: torch.Size([32, 269])\n",
      "extended_attention_mask: torch.Size([32, 1, 1, 269])\n",
      "[tensor([[[-0.0327,  0.4961, -1.0498,  ...,  0.5652, -0.4733,  0.6542],\n",
      "         [-0.0667,  0.4875, -1.0481,  ...,  0.4931, -0.4858,  0.7454],\n",
      "         [-0.0418,  0.4968, -1.0184,  ...,  0.5328, -0.4763,  0.6323],\n",
      "         ...,\n",
      "         [-0.0621,  0.4896, -0.9943,  ...,  0.3946, -0.4277,  0.6624],\n",
      "         [-0.0621,  0.4896, -0.9943,  ...,  0.3946, -0.4277,  0.6624],\n",
      "         [-0.0621,  0.4896, -0.9943,  ...,  0.3946, -0.4277,  0.6624]],\n",
      "\n",
      "        [[ 0.4442,  0.6576, -1.0839,  ...,  0.6760, -0.2745,  0.7672],\n",
      "         [ 0.4292,  0.6781, -1.0742,  ...,  0.6408, -0.2788,  0.7749],\n",
      "         [ 0.4488,  0.6668, -1.0635,  ...,  0.6466, -0.2656,  0.7589],\n",
      "         ...,\n",
      "         [ 0.4351,  0.6676, -1.0223,  ...,  0.4955, -0.2397,  0.7779],\n",
      "         [ 0.4351,  0.6676, -1.0223,  ...,  0.4955, -0.2397,  0.7779],\n",
      "         [ 0.4351,  0.6676, -1.0223,  ...,  0.4955, -0.2397,  0.7779]],\n",
      "\n",
      "        [[ 0.1451,  0.5135, -1.0776,  ...,  0.5953, -0.4176,  0.6365],\n",
      "         [ 0.1282,  0.5345, -1.0632,  ...,  0.5769, -0.4241,  0.6403],\n",
      "         [ 0.1492,  0.5081, -1.0300,  ...,  0.5244, -0.4050,  0.6478],\n",
      "         ...,\n",
      "         [ 0.1241,  0.5182, -1.0194,  ...,  0.4229, -0.3753,  0.6436],\n",
      "         [ 0.1241,  0.5182, -1.0194,  ...,  0.4229, -0.3753,  0.6436],\n",
      "         [ 0.1241,  0.5182, -1.0194,  ...,  0.4229, -0.3753,  0.6436]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5623,  0.2814, -0.9774,  ...,  0.3542, -0.6996,  0.4056],\n",
      "         [-0.5746,  0.2770, -0.9702,  ...,  0.3075, -0.7074,  0.4419],\n",
      "         [-0.6128,  0.2937, -0.9482,  ...,  0.3730, -0.7155,  0.4171],\n",
      "         ...,\n",
      "         [-0.6180,  0.2665, -0.9304,  ...,  0.2006, -0.6419,  0.4122],\n",
      "         [-0.6180,  0.2665, -0.9304,  ...,  0.2006, -0.6419,  0.4122],\n",
      "         [-0.6180,  0.2665, -0.9304,  ...,  0.2006, -0.6419,  0.4122]],\n",
      "\n",
      "        [[-0.5866,  0.2839, -0.9331,  ...,  0.3244, -0.6652,  0.5005],\n",
      "         [-0.5739,  0.3064, -0.9284,  ...,  0.2948, -0.6666,  0.5080],\n",
      "         [-0.5933,  0.2483, -0.8787,  ...,  0.2187, -0.6915,  0.5149],\n",
      "         ...,\n",
      "         [-0.6256,  0.2694, -0.8888,  ...,  0.1586, -0.6141,  0.5138],\n",
      "         [-0.6256,  0.2694, -0.8888,  ...,  0.1586, -0.6141,  0.5138],\n",
      "         [-0.6256,  0.2694, -0.8888,  ...,  0.1586, -0.6141,  0.5138]],\n",
      "\n",
      "        [[-0.0649,  0.4502, -1.0670,  ...,  0.5309, -0.5068,  0.6099],\n",
      "         [-0.1113,  0.4500, -1.0605,  ...,  0.4568, -0.5156,  0.7099],\n",
      "         [-0.1159,  0.4630, -1.0578,  ...,  0.4583, -0.4993,  0.6953],\n",
      "         ...,\n",
      "         [-0.0980,  0.4466, -1.0095,  ...,  0.3631, -0.4588,  0.6154],\n",
      "         [-0.0980,  0.4466, -1.0095,  ...,  0.3631, -0.4588,  0.6154],\n",
      "         [-0.0980,  0.4466, -1.0095,  ...,  0.3631, -0.4588,  0.6154]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5010,  0.6833,  0.5264,  ..., -0.1636,  0.8219, -0.4715],\n",
      "         [ 0.5009,  0.6833,  0.5264,  ..., -0.1636,  0.8219, -0.4715],\n",
      "         [ 0.5010,  0.6833,  0.5264,  ..., -0.1637,  0.8219, -0.4715],\n",
      "         ...,\n",
      "         [ 0.5010,  0.6833,  0.5267,  ..., -0.1636,  0.8218, -0.4715],\n",
      "         [ 0.5010,  0.6833,  0.5267,  ..., -0.1636,  0.8218, -0.4715],\n",
      "         [ 0.5010,  0.6833,  0.5267,  ..., -0.1636,  0.8218, -0.4715]],\n",
      "\n",
      "        [[ 0.5528,  0.5349,  0.6283,  ..., -0.1629,  0.9538, -0.3439],\n",
      "         [ 0.5528,  0.5349,  0.6282,  ..., -0.1630,  0.9538, -0.3438],\n",
      "         [ 0.5529,  0.5349,  0.6283,  ..., -0.1629,  0.9538, -0.3438],\n",
      "         ...,\n",
      "         [ 0.5530,  0.5348,  0.6286,  ..., -0.1628,  0.9537, -0.3439],\n",
      "         [ 0.5530,  0.5348,  0.6286,  ..., -0.1628,  0.9537, -0.3439],\n",
      "         [ 0.5530,  0.5348,  0.6286,  ..., -0.1628,  0.9537, -0.3439]],\n",
      "\n",
      "        [[ 0.5807,  0.5998,  0.5456,  ..., -0.1868,  0.8498, -0.5042],\n",
      "         [ 0.5807,  0.5998,  0.5455,  ..., -0.1868,  0.8498, -0.5041],\n",
      "         [ 0.5808,  0.5998,  0.5455,  ..., -0.1869,  0.8499, -0.5042],\n",
      "         ...,\n",
      "         [ 0.5808,  0.5998,  0.5459,  ..., -0.1868,  0.8497, -0.5042],\n",
      "         [ 0.5808,  0.5998,  0.5459,  ..., -0.1868,  0.8497, -0.5042],\n",
      "         [ 0.5808,  0.5998,  0.5459,  ..., -0.1868,  0.8497, -0.5042]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5367,  0.7281,  0.2849,  ..., -0.2516,  0.6371, -0.8060],\n",
      "         [ 0.5367,  0.7281,  0.2850,  ..., -0.2516,  0.6371, -0.8060],\n",
      "         [ 0.5367,  0.7281,  0.2848,  ..., -0.2516,  0.6372, -0.8061],\n",
      "         ...,\n",
      "         [ 0.5367,  0.7279,  0.2854,  ..., -0.2518,  0.6369, -0.8060],\n",
      "         [ 0.5367,  0.7279,  0.2854,  ..., -0.2518,  0.6369, -0.8060],\n",
      "         [ 0.5367,  0.7279,  0.2854,  ..., -0.2518,  0.6369, -0.8060]],\n",
      "\n",
      "        [[ 0.5068,  0.7282,  0.3140,  ..., -0.3090,  0.7421, -0.8548],\n",
      "         [ 0.5069,  0.7282,  0.3140,  ..., -0.3089,  0.7420, -0.8548],\n",
      "         [ 0.5069,  0.7282,  0.3143,  ..., -0.3090,  0.7420, -0.8547],\n",
      "         ...,\n",
      "         [ 0.5069,  0.7280,  0.3144,  ..., -0.3091,  0.7419, -0.8547],\n",
      "         [ 0.5069,  0.7280,  0.3144,  ..., -0.3091,  0.7419, -0.8547],\n",
      "         [ 0.5069,  0.7280,  0.3144,  ..., -0.3091,  0.7419, -0.8547]],\n",
      "\n",
      "        [[ 0.5655,  0.6562,  0.4783,  ..., -0.1977,  0.8182, -0.5565],\n",
      "         [ 0.5655,  0.6561,  0.4783,  ..., -0.1977,  0.8183, -0.5565],\n",
      "         [ 0.5655,  0.6561,  0.4784,  ..., -0.1977,  0.8183, -0.5564],\n",
      "         ...,\n",
      "         [ 0.5656,  0.6561,  0.4786,  ..., -0.1976,  0.8181, -0.5565],\n",
      "         [ 0.5656,  0.6561,  0.4786,  ..., -0.1976,  0.8181, -0.5565],\n",
      "         [ 0.5656,  0.6561,  0.4786,  ..., -0.1976,  0.8181, -0.5565]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6211,  0.7445, -1.1311,  ...,  0.8187,  1.5676, -1.0733],\n",
      "         [-0.6211,  0.7445, -1.1311,  ...,  0.8187,  1.5676, -1.0733],\n",
      "         [-0.6211,  0.7445, -1.1311,  ...,  0.8187,  1.5676, -1.0733],\n",
      "         ...,\n",
      "         [-0.6211,  0.7445, -1.1311,  ...,  0.8187,  1.5676, -1.0733],\n",
      "         [-0.6211,  0.7445, -1.1311,  ...,  0.8187,  1.5676, -1.0733],\n",
      "         [-0.6211,  0.7445, -1.1311,  ...,  0.8187,  1.5676, -1.0733]],\n",
      "\n",
      "        [[-0.7229,  0.9772, -0.9820,  ...,  0.8083,  1.7730, -0.9358],\n",
      "         [-0.7229,  0.9772, -0.9820,  ...,  0.8083,  1.7730, -0.9358],\n",
      "         [-0.7229,  0.9772, -0.9820,  ...,  0.8083,  1.7730, -0.9358],\n",
      "         ...,\n",
      "         [-0.7229,  0.9772, -0.9820,  ...,  0.8083,  1.7730, -0.9358],\n",
      "         [-0.7229,  0.9772, -0.9820,  ...,  0.8083,  1.7730, -0.9358],\n",
      "         [-0.7229,  0.9772, -0.9820,  ...,  0.8083,  1.7730, -0.9358]],\n",
      "\n",
      "        [[-0.6635,  0.8675, -1.0910,  ...,  0.8479,  1.6219, -1.0539],\n",
      "         [-0.6635,  0.8675, -1.0910,  ...,  0.8479,  1.6219, -1.0539],\n",
      "         [-0.6635,  0.8675, -1.0910,  ...,  0.8479,  1.6219, -1.0539],\n",
      "         ...,\n",
      "         [-0.6635,  0.8675, -1.0910,  ...,  0.8479,  1.6219, -1.0539],\n",
      "         [-0.6635,  0.8675, -1.0910,  ...,  0.8479,  1.6219, -1.0539],\n",
      "         [-0.6635,  0.8675, -1.0910,  ...,  0.8479,  1.6219, -1.0539]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4971,  0.4376, -1.3543,  ...,  0.9096,  1.1198, -1.2850],\n",
      "         [-0.4971,  0.4376, -1.3543,  ...,  0.9096,  1.1198, -1.2850],\n",
      "         [-0.4971,  0.4376, -1.3543,  ...,  0.9096,  1.1198, -1.2850],\n",
      "         ...,\n",
      "         [-0.4971,  0.4376, -1.3543,  ...,  0.9096,  1.1198, -1.2850],\n",
      "         [-0.4971,  0.4376, -1.3543,  ...,  0.9096,  1.1198, -1.2850],\n",
      "         [-0.4971,  0.4376, -1.3543,  ...,  0.9096,  1.1198, -1.2850]],\n",
      "\n",
      "        [[-0.5118,  0.4049, -1.2876,  ...,  0.9315,  1.0772, -1.2497],\n",
      "         [-0.5118,  0.4049, -1.2876,  ...,  0.9315,  1.0772, -1.2497],\n",
      "         [-0.5118,  0.4049, -1.2876,  ...,  0.9315,  1.0772, -1.2497],\n",
      "         ...,\n",
      "         [-0.5118,  0.4049, -1.2876,  ...,  0.9315,  1.0772, -1.2497],\n",
      "         [-0.5118,  0.4049, -1.2876,  ...,  0.9315,  1.0772, -1.2497],\n",
      "         [-0.5118,  0.4049, -1.2876,  ...,  0.9315,  1.0772, -1.2497]],\n",
      "\n",
      "        [[-0.6389,  0.7357, -1.1641,  ...,  0.8763,  1.5150, -1.0952],\n",
      "         [-0.6389,  0.7357, -1.1641,  ...,  0.8763,  1.5150, -1.0952],\n",
      "         [-0.6389,  0.7357, -1.1641,  ...,  0.8763,  1.5150, -1.0952],\n",
      "         ...,\n",
      "         [-0.6389,  0.7357, -1.1641,  ...,  0.8763,  1.5150, -1.0952],\n",
      "         [-0.6389,  0.7357, -1.1641,  ...,  0.8763,  1.5150, -1.0952],\n",
      "         [-0.6389,  0.7357, -1.1641,  ...,  0.8763,  1.5150, -1.0952]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7982, -0.8294, -0.5902,  ...,  0.1002, -0.3333,  0.2870],\n",
      "         [-0.7982, -0.8294, -0.5902,  ...,  0.1002, -0.3333,  0.2870],\n",
      "         [-0.7982, -0.8294, -0.5902,  ...,  0.1002, -0.3333,  0.2870],\n",
      "         ...,\n",
      "         [-0.7982, -0.8294, -0.5902,  ...,  0.1002, -0.3333,  0.2870],\n",
      "         [-0.7982, -0.8294, -0.5902,  ...,  0.1002, -0.3333,  0.2870],\n",
      "         [-0.7982, -0.8294, -0.5902,  ...,  0.1002, -0.3333,  0.2870]],\n",
      "\n",
      "        [[-0.6295, -0.7766, -0.3325,  ..., -0.1318, -0.2408,  0.2111],\n",
      "         [-0.6295, -0.7766, -0.3325,  ..., -0.1318, -0.2408,  0.2111],\n",
      "         [-0.6295, -0.7766, -0.3325,  ..., -0.1318, -0.2408,  0.2111],\n",
      "         ...,\n",
      "         [-0.6295, -0.7766, -0.3325,  ..., -0.1318, -0.2408,  0.2111],\n",
      "         [-0.6295, -0.7766, -0.3325,  ..., -0.1318, -0.2408,  0.2111],\n",
      "         [-0.6295, -0.7766, -0.3325,  ..., -0.1318, -0.2408,  0.2111]],\n",
      "\n",
      "        [[-0.7405, -0.8498, -0.5002,  ..., -0.0052, -0.2590,  0.2273],\n",
      "         [-0.7405, -0.8498, -0.5002,  ..., -0.0052, -0.2590,  0.2273],\n",
      "         [-0.7405, -0.8498, -0.5002,  ..., -0.0052, -0.2590,  0.2273],\n",
      "         ...,\n",
      "         [-0.7405, -0.8498, -0.5002,  ..., -0.0052, -0.2590,  0.2273],\n",
      "         [-0.7405, -0.8498, -0.5002,  ..., -0.0052, -0.2590,  0.2273],\n",
      "         [-0.7405, -0.8498, -0.5002,  ..., -0.0052, -0.2590,  0.2273]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9522, -0.9617, -0.8717,  ...,  0.3640, -0.3082,  0.3376],\n",
      "         [-0.9522, -0.9617, -0.8717,  ...,  0.3640, -0.3082,  0.3376],\n",
      "         [-0.9522, -0.9617, -0.8717,  ...,  0.3640, -0.3082,  0.3376],\n",
      "         ...,\n",
      "         [-0.9522, -0.9617, -0.8717,  ...,  0.3640, -0.3082,  0.3376],\n",
      "         [-0.9522, -0.9617, -0.8717,  ...,  0.3640, -0.3082,  0.3376],\n",
      "         [-0.9522, -0.9617, -0.8717,  ...,  0.3640, -0.3082,  0.3376]],\n",
      "\n",
      "        [[-0.8783, -0.9679, -0.8119,  ...,  0.3464, -0.2459,  0.3629],\n",
      "         [-0.8783, -0.9679, -0.8119,  ...,  0.3464, -0.2459,  0.3629],\n",
      "         [-0.8783, -0.9679, -0.8119,  ...,  0.3464, -0.2459,  0.3629],\n",
      "         ...,\n",
      "         [-0.8783, -0.9679, -0.8119,  ...,  0.3464, -0.2459,  0.3629],\n",
      "         [-0.8783, -0.9679, -0.8119,  ...,  0.3464, -0.2459,  0.3629],\n",
      "         [-0.8783, -0.9679, -0.8119,  ...,  0.3464, -0.2459,  0.3629]],\n",
      "\n",
      "        [[-0.8143, -0.8763, -0.5990,  ...,  0.1026, -0.2698,  0.2713],\n",
      "         [-0.8143, -0.8763, -0.5990,  ...,  0.1026, -0.2698,  0.2713],\n",
      "         [-0.8143, -0.8763, -0.5990,  ...,  0.1026, -0.2698,  0.2713],\n",
      "         ...,\n",
      "         [-0.8143, -0.8763, -0.5990,  ...,  0.1026, -0.2698,  0.2713],\n",
      "         [-0.8143, -0.8763, -0.5990,  ...,  0.1026, -0.2698,  0.2713],\n",
      "         [-0.8143, -0.8763, -0.5990,  ...,  0.1026, -0.2698,  0.2713]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1767, -0.5972, -1.5488,  ...,  1.2070, -0.2401,  0.0461],\n",
      "         [-0.1767, -0.5972, -1.5488,  ...,  1.2070, -0.2401,  0.0461],\n",
      "         [-0.1767, -0.5972, -1.5488,  ...,  1.2070, -0.2401,  0.0461],\n",
      "         ...,\n",
      "         [-0.1767, -0.5972, -1.5488,  ...,  1.2070, -0.2401,  0.0461],\n",
      "         [-0.1767, -0.5972, -1.5488,  ...,  1.2070, -0.2401,  0.0461],\n",
      "         [-0.1767, -0.5972, -1.5488,  ...,  1.2070, -0.2401,  0.0461]],\n",
      "\n",
      "        [[-0.4486, -0.5583, -1.2431,  ...,  1.4486, -0.0471, -0.4189],\n",
      "         [-0.4486, -0.5583, -1.2431,  ...,  1.4486, -0.0471, -0.4189],\n",
      "         [-0.4486, -0.5583, -1.2431,  ...,  1.4486, -0.0471, -0.4189],\n",
      "         ...,\n",
      "         [-0.4486, -0.5583, -1.2431,  ...,  1.4486, -0.0471, -0.4189],\n",
      "         [-0.4486, -0.5583, -1.2431,  ...,  1.4486, -0.0471, -0.4189],\n",
      "         [-0.4486, -0.5583, -1.2431,  ...,  1.4486, -0.0471, -0.4189]],\n",
      "\n",
      "        [[-0.2776, -0.5451, -1.4057,  ...,  1.3166, -0.2301, -0.0690],\n",
      "         [-0.2776, -0.5451, -1.4057,  ...,  1.3166, -0.2301, -0.0690],\n",
      "         [-0.2776, -0.5451, -1.4057,  ...,  1.3166, -0.2301, -0.0690],\n",
      "         ...,\n",
      "         [-0.2776, -0.5451, -1.4057,  ...,  1.3166, -0.2301, -0.0690],\n",
      "         [-0.2776, -0.5451, -1.4057,  ...,  1.3166, -0.2301, -0.0690],\n",
      "         [-0.2776, -0.5451, -1.4057,  ...,  1.3166, -0.2301, -0.0690]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1114, -0.5803, -1.7228,  ...,  0.9035, -0.4464,  0.5297],\n",
      "         [ 0.1114, -0.5803, -1.7228,  ...,  0.9035, -0.4464,  0.5297],\n",
      "         [ 0.1114, -0.5803, -1.7228,  ...,  0.9035, -0.4464,  0.5297],\n",
      "         ...,\n",
      "         [ 0.1114, -0.5803, -1.7228,  ...,  0.9035, -0.4464,  0.5297],\n",
      "         [ 0.1114, -0.5803, -1.7228,  ...,  0.9035, -0.4464,  0.5297],\n",
      "         [ 0.1114, -0.5803, -1.7228,  ...,  0.9035, -0.4464,  0.5297]],\n",
      "\n",
      "        [[ 0.0400, -0.6273, -1.6075,  ...,  0.9223, -0.3592,  0.4836],\n",
      "         [ 0.0400, -0.6273, -1.6075,  ...,  0.9223, -0.3592,  0.4836],\n",
      "         [ 0.0400, -0.6273, -1.6075,  ...,  0.9223, -0.3592,  0.4836],\n",
      "         ...,\n",
      "         [ 0.0400, -0.6273, -1.6075,  ...,  0.9223, -0.3592,  0.4836],\n",
      "         [ 0.0400, -0.6273, -1.6075,  ...,  0.9223, -0.3592,  0.4836],\n",
      "         [ 0.0400, -0.6273, -1.6075,  ...,  0.9223, -0.3592,  0.4836]],\n",
      "\n",
      "        [[-0.1482, -0.5843, -1.5038,  ...,  1.2179, -0.2729,  0.0644],\n",
      "         [-0.1482, -0.5843, -1.5038,  ...,  1.2179, -0.2729,  0.0644],\n",
      "         [-0.1482, -0.5843, -1.5038,  ...,  1.2179, -0.2729,  0.0644],\n",
      "         ...,\n",
      "         [-0.1482, -0.5843, -1.5038,  ...,  1.2179, -0.2729,  0.0644],\n",
      "         [-0.1482, -0.5843, -1.5038,  ...,  1.2179, -0.2729,  0.0644],\n",
      "         [-0.1482, -0.5843, -1.5038,  ...,  1.2179, -0.2729,  0.0644]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3059, -2.2886, -1.3273,  ...,  0.0401,  0.6433, -0.5923],\n",
      "         [-0.3059, -2.2886, -1.3273,  ...,  0.0401,  0.6433, -0.5923],\n",
      "         [-0.3059, -2.2886, -1.3273,  ...,  0.0401,  0.6433, -0.5923],\n",
      "         ...,\n",
      "         [-0.3059, -2.2886, -1.3273,  ...,  0.0401,  0.6433, -0.5923],\n",
      "         [-0.3059, -2.2886, -1.3273,  ...,  0.0401,  0.6433, -0.5923],\n",
      "         [-0.3059, -2.2886, -1.3273,  ...,  0.0401,  0.6433, -0.5923]],\n",
      "\n",
      "        [[-0.0904, -1.9017, -1.5891,  ..., -0.1224,  0.2759, -0.3013],\n",
      "         [-0.0904, -1.9017, -1.5891,  ..., -0.1224,  0.2759, -0.3013],\n",
      "         [-0.0904, -1.9017, -1.5891,  ..., -0.1224,  0.2759, -0.3013],\n",
      "         ...,\n",
      "         [-0.0904, -1.9017, -1.5891,  ..., -0.1224,  0.2759, -0.3013],\n",
      "         [-0.0904, -1.9017, -1.5891,  ..., -0.1224,  0.2759, -0.3013],\n",
      "         [-0.0904, -1.9017, -1.5891,  ..., -0.1224,  0.2759, -0.3013]],\n",
      "\n",
      "        [[-0.2906, -2.1938, -1.3763,  ...,  0.0624,  0.5174, -0.4650],\n",
      "         [-0.2906, -2.1938, -1.3763,  ...,  0.0624,  0.5174, -0.4650],\n",
      "         [-0.2906, -2.1938, -1.3763,  ...,  0.0624,  0.5174, -0.4650],\n",
      "         ...,\n",
      "         [-0.2906, -2.1938, -1.3763,  ...,  0.0624,  0.5174, -0.4650],\n",
      "         [-0.2906, -2.1938, -1.3763,  ...,  0.0624,  0.5174, -0.4650],\n",
      "         [-0.2906, -2.1938, -1.3763,  ...,  0.0624,  0.5174, -0.4650]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6629, -2.6075, -0.9843,  ...,  0.3267,  0.9279, -0.8482],\n",
      "         [-0.6629, -2.6075, -0.9843,  ...,  0.3267,  0.9279, -0.8482],\n",
      "         [-0.6629, -2.6075, -0.9843,  ...,  0.3267,  0.9279, -0.8482],\n",
      "         ...,\n",
      "         [-0.6629, -2.6075, -0.9843,  ...,  0.3267,  0.9279, -0.8482],\n",
      "         [-0.6629, -2.6075, -0.9843,  ...,  0.3267,  0.9279, -0.8482],\n",
      "         [-0.6629, -2.6075, -0.9843,  ...,  0.3267,  0.9279, -0.8482]],\n",
      "\n",
      "        [[-0.6015, -2.4646, -1.0446,  ...,  0.2696,  0.7894, -0.7698],\n",
      "         [-0.6015, -2.4646, -1.0446,  ...,  0.2696,  0.7894, -0.7698],\n",
      "         [-0.6015, -2.4646, -1.0446,  ...,  0.2696,  0.7894, -0.7698],\n",
      "         ...,\n",
      "         [-0.6015, -2.4646, -1.0446,  ...,  0.2696,  0.7894, -0.7698],\n",
      "         [-0.6015, -2.4646, -1.0446,  ...,  0.2696,  0.7894, -0.7698],\n",
      "         [-0.6015, -2.4646, -1.0446,  ...,  0.2696,  0.7894, -0.7698]],\n",
      "\n",
      "        [[-0.3773, -2.3181, -1.2989,  ...,  0.1075,  0.6341, -0.5858],\n",
      "         [-0.3773, -2.3181, -1.2989,  ...,  0.1075,  0.6341, -0.5858],\n",
      "         [-0.3773, -2.3181, -1.2989,  ...,  0.1075,  0.6341, -0.5858],\n",
      "         ...,\n",
      "         [-0.3773, -2.3181, -1.2989,  ...,  0.1075,  0.6341, -0.5858],\n",
      "         [-0.3773, -2.3181, -1.2989,  ...,  0.1075,  0.6341, -0.5858],\n",
      "         [-0.3773, -2.3181, -1.2989,  ...,  0.1075,  0.6341, -0.5858]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.1153, -0.3691, -0.3203,  ...,  0.5844, -0.8372, -1.1189],\n",
      "         [ 1.1153, -0.3692, -0.3203,  ...,  0.5844, -0.8372, -1.1189],\n",
      "         [ 1.1153, -0.3692, -0.3203,  ...,  0.5844, -0.8372, -1.1189],\n",
      "         ...,\n",
      "         [ 1.1153, -0.3692, -0.3203,  ...,  0.5844, -0.8372, -1.1189],\n",
      "         [ 1.1153, -0.3692, -0.3203,  ...,  0.5844, -0.8372, -1.1189],\n",
      "         [ 1.1153, -0.3692, -0.3203,  ...,  0.5844, -0.8372, -1.1189]],\n",
      "\n",
      "        [[ 1.0742, -0.6201, -0.4850,  ...,  0.6041, -0.3841, -1.1556],\n",
      "         [ 1.0742, -0.6201, -0.4850,  ...,  0.6041, -0.3841, -1.1556],\n",
      "         [ 1.0742, -0.6201, -0.4850,  ...,  0.6041, -0.3841, -1.1556],\n",
      "         ...,\n",
      "         [ 1.0742, -0.6201, -0.4850,  ...,  0.6041, -0.3841, -1.1556],\n",
      "         [ 1.0742, -0.6201, -0.4850,  ...,  0.6041, -0.3841, -1.1556],\n",
      "         [ 1.0742, -0.6201, -0.4850,  ...,  0.6041, -0.3841, -1.1556]],\n",
      "\n",
      "        [[ 1.1418, -0.4009, -0.3587,  ...,  0.5532, -0.6859, -1.1734],\n",
      "         [ 1.1418, -0.4009, -0.3587,  ...,  0.5532, -0.6859, -1.1734],\n",
      "         [ 1.1418, -0.4009, -0.3587,  ...,  0.5532, -0.6859, -1.1734],\n",
      "         ...,\n",
      "         [ 1.1418, -0.4009, -0.3587,  ...,  0.5532, -0.6859, -1.1734],\n",
      "         [ 1.1418, -0.4009, -0.3587,  ...,  0.5532, -0.6859, -1.1734],\n",
      "         [ 1.1418, -0.4009, -0.3587,  ...,  0.5532, -0.6859, -1.1734]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1024,  0.0704, -0.1226,  ...,  0.4697, -1.1940, -1.0373],\n",
      "         [ 1.1024,  0.0704, -0.1226,  ...,  0.4697, -1.1940, -1.0373],\n",
      "         [ 1.1024,  0.0704, -0.1226,  ...,  0.4697, -1.1940, -1.0373],\n",
      "         ...,\n",
      "         [ 1.1024,  0.0704, -0.1226,  ...,  0.4697, -1.1940, -1.0373],\n",
      "         [ 1.1024,  0.0704, -0.1226,  ...,  0.4697, -1.1940, -1.0373],\n",
      "         [ 1.1024,  0.0704, -0.1226,  ...,  0.4697, -1.1940, -1.0373]],\n",
      "\n",
      "        [[ 1.0857,  0.1013, -0.1164,  ...,  0.5944, -1.0765, -1.0264],\n",
      "         [ 1.0857,  0.1013, -0.1164,  ...,  0.5944, -1.0765, -1.0264],\n",
      "         [ 1.0857,  0.1013, -0.1164,  ...,  0.5944, -1.0765, -1.0264],\n",
      "         ...,\n",
      "         [ 1.0857,  0.1013, -0.1164,  ...,  0.5944, -1.0765, -1.0264],\n",
      "         [ 1.0857,  0.1013, -0.1164,  ...,  0.5944, -1.0765, -1.0264],\n",
      "         [ 1.0857,  0.1013, -0.1164,  ...,  0.5944, -1.0765, -1.0264]],\n",
      "\n",
      "        [[ 1.1405, -0.2979, -0.3076,  ...,  0.5755, -0.8304, -1.1312],\n",
      "         [ 1.1405, -0.2979, -0.3076,  ...,  0.5755, -0.8304, -1.1312],\n",
      "         [ 1.1405, -0.2979, -0.3076,  ...,  0.5755, -0.8304, -1.1312],\n",
      "         ...,\n",
      "         [ 1.1405, -0.2979, -0.3076,  ...,  0.5755, -0.8304, -1.1312],\n",
      "         [ 1.1405, -0.2979, -0.3076,  ...,  0.5755, -0.8304, -1.1312],\n",
      "         [ 1.1405, -0.2979, -0.3076,  ...,  0.5755, -0.8304, -1.1312]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3290,  1.2795, -0.9233,  ..., -0.9486, -0.6309,  1.4626],\n",
      "         [-0.3290,  1.2795, -0.9233,  ..., -0.9486, -0.6309,  1.4626],\n",
      "         [-0.3290,  1.2795, -0.9233,  ..., -0.9486, -0.6309,  1.4626],\n",
      "         ...,\n",
      "         [-0.3290,  1.2795, -0.9233,  ..., -0.9486, -0.6309,  1.4626],\n",
      "         [-0.3290,  1.2795, -0.9233,  ..., -0.9486, -0.6309,  1.4626],\n",
      "         [-0.3290,  1.2795, -0.9233,  ..., -0.9486, -0.6309,  1.4626]],\n",
      "\n",
      "        [[-0.1395,  1.3120, -0.9823,  ..., -1.3456, -0.5115,  1.2886],\n",
      "         [-0.1395,  1.3120, -0.9823,  ..., -1.3456, -0.5115,  1.2886],\n",
      "         [-0.1395,  1.3120, -0.9823,  ..., -1.3456, -0.5115,  1.2886],\n",
      "         ...,\n",
      "         [-0.1395,  1.3120, -0.9823,  ..., -1.3456, -0.5115,  1.2886],\n",
      "         [-0.1395,  1.3120, -0.9823,  ..., -1.3456, -0.5115,  1.2886],\n",
      "         [-0.1395,  1.3120, -0.9823,  ..., -1.3456, -0.5115,  1.2886]],\n",
      "\n",
      "        [[-0.2308,  1.2652, -0.8949,  ..., -1.1034, -0.5781,  1.3990],\n",
      "         [-0.2308,  1.2652, -0.8949,  ..., -1.1034, -0.5781,  1.3990],\n",
      "         [-0.2308,  1.2652, -0.8949,  ..., -1.1034, -0.5781,  1.3990],\n",
      "         ...,\n",
      "         [-0.2308,  1.2652, -0.8949,  ..., -1.1034, -0.5781,  1.3990],\n",
      "         [-0.2308,  1.2652, -0.8949,  ..., -1.1034, -0.5781,  1.3990],\n",
      "         [-0.2308,  1.2652, -0.8949,  ..., -1.1034, -0.5781,  1.3990]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4616,  1.1157, -0.7156,  ..., -0.5497, -0.7626,  1.5837],\n",
      "         [-0.4616,  1.1157, -0.7156,  ..., -0.5497, -0.7626,  1.5837],\n",
      "         [-0.4616,  1.1157, -0.7156,  ..., -0.5497, -0.7626,  1.5837],\n",
      "         ...,\n",
      "         [-0.4616,  1.1157, -0.7156,  ..., -0.5497, -0.7626,  1.5837],\n",
      "         [-0.4616,  1.1157, -0.7156,  ..., -0.5497, -0.7626,  1.5837],\n",
      "         [-0.4616,  1.1157, -0.7156,  ..., -0.5497, -0.7626,  1.5837]],\n",
      "\n",
      "        [[-0.5077,  1.1778, -0.7814,  ..., -0.6980, -0.8946,  1.5933],\n",
      "         [-0.5077,  1.1778, -0.7814,  ..., -0.6980, -0.8946,  1.5933],\n",
      "         [-0.5077,  1.1778, -0.7814,  ..., -0.6980, -0.8946,  1.5933],\n",
      "         ...,\n",
      "         [-0.5077,  1.1778, -0.7814,  ..., -0.6980, -0.8946,  1.5933],\n",
      "         [-0.5077,  1.1778, -0.7814,  ..., -0.6980, -0.8946,  1.5933],\n",
      "         [-0.5077,  1.1778, -0.7814,  ..., -0.6980, -0.8946,  1.5933]],\n",
      "\n",
      "        [[-0.3300,  1.2211, -0.8672,  ..., -0.9607, -0.6438,  1.4840],\n",
      "         [-0.3300,  1.2211, -0.8672,  ..., -0.9607, -0.6438,  1.4840],\n",
      "         [-0.3300,  1.2211, -0.8672,  ..., -0.9607, -0.6438,  1.4840],\n",
      "         ...,\n",
      "         [-0.3300,  1.2211, -0.8672,  ..., -0.9607, -0.6438,  1.4840],\n",
      "         [-0.3300,  1.2211, -0.8672,  ..., -0.9607, -0.6438,  1.4841],\n",
      "         [-0.3300,  1.2211, -0.8672,  ..., -0.9607, -0.6438,  1.4840]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2527, -1.0940, -0.5504,  ...,  1.1899,  0.7412,  0.3832],\n",
      "         [ 0.2527, -1.0940, -0.5504,  ...,  1.1899,  0.7412,  0.3832],\n",
      "         [ 0.2527, -1.0940, -0.5504,  ...,  1.1899,  0.7412,  0.3832],\n",
      "         ...,\n",
      "         [ 0.2527, -1.0940, -0.5504,  ...,  1.1899,  0.7412,  0.3832],\n",
      "         [ 0.2527, -1.0940, -0.5504,  ...,  1.1899,  0.7412,  0.3832],\n",
      "         [ 0.2527, -1.0940, -0.5504,  ...,  1.1899,  0.7412,  0.3832]],\n",
      "\n",
      "        [[ 0.0716, -1.0318, -0.2941,  ...,  1.0910,  1.0003,  0.2880],\n",
      "         [ 0.0716, -1.0318, -0.2941,  ...,  1.0910,  1.0003,  0.2880],\n",
      "         [ 0.0716, -1.0318, -0.2941,  ...,  1.0910,  1.0003,  0.2880],\n",
      "         ...,\n",
      "         [ 0.0716, -1.0318, -0.2941,  ...,  1.0910,  1.0003,  0.2880],\n",
      "         [ 0.0716, -1.0317, -0.2941,  ...,  1.0910,  1.0003,  0.2880],\n",
      "         [ 0.0716, -1.0317, -0.2941,  ...,  1.0910,  1.0003,  0.2880]],\n",
      "\n",
      "        [[ 0.2422, -1.0535, -0.4818,  ...,  1.2018,  0.8506,  0.3662],\n",
      "         [ 0.2422, -1.0535, -0.4818,  ...,  1.2019,  0.8506,  0.3662],\n",
      "         [ 0.2422, -1.0535, -0.4818,  ...,  1.2019,  0.8506,  0.3662],\n",
      "         ...,\n",
      "         [ 0.2422, -1.0535, -0.4818,  ...,  1.2018,  0.8506,  0.3662],\n",
      "         [ 0.2422, -1.0535, -0.4818,  ...,  1.2018,  0.8506,  0.3662],\n",
      "         [ 0.2422, -1.0535, -0.4818,  ...,  1.2018,  0.8506,  0.3662]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4950, -1.0921, -0.7962,  ...,  1.3266,  0.5052,  0.4724],\n",
      "         [ 0.4950, -1.0921, -0.7962,  ...,  1.3266,  0.5052,  0.4724],\n",
      "         [ 0.4950, -1.0921, -0.7962,  ...,  1.3266,  0.5052,  0.4724],\n",
      "         ...,\n",
      "         [ 0.4950, -1.0921, -0.7962,  ...,  1.3266,  0.5052,  0.4724],\n",
      "         [ 0.4950, -1.0921, -0.7962,  ...,  1.3266,  0.5052,  0.4724],\n",
      "         [ 0.4950, -1.0921, -0.7962,  ...,  1.3266,  0.5052,  0.4724]],\n",
      "\n",
      "        [[ 0.5342, -1.0219, -0.7331,  ...,  1.3716,  0.5303,  0.5183],\n",
      "         [ 0.5342, -1.0219, -0.7331,  ...,  1.3716,  0.5303,  0.5183],\n",
      "         [ 0.5342, -1.0219, -0.7331,  ...,  1.3716,  0.5303,  0.5183],\n",
      "         ...,\n",
      "         [ 0.5342, -1.0219, -0.7331,  ...,  1.3716,  0.5303,  0.5183],\n",
      "         [ 0.5342, -1.0219, -0.7331,  ...,  1.3716,  0.5303,  0.5183],\n",
      "         [ 0.5342, -1.0219, -0.7331,  ...,  1.3716,  0.5303,  0.5183]],\n",
      "\n",
      "        [[ 0.2956, -1.0648, -0.5681,  ...,  1.2442,  0.7487,  0.4076],\n",
      "         [ 0.2956, -1.0648, -0.5681,  ...,  1.2442,  0.7487,  0.4076],\n",
      "         [ 0.2956, -1.0648, -0.5681,  ...,  1.2442,  0.7487,  0.4076],\n",
      "         ...,\n",
      "         [ 0.2956, -1.0648, -0.5681,  ...,  1.2442,  0.7487,  0.4076],\n",
      "         [ 0.2956, -1.0648, -0.5681,  ...,  1.2442,  0.7487,  0.4076],\n",
      "         [ 0.2956, -1.0648, -0.5681,  ...,  1.2442,  0.7487,  0.4076]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0301, -1.9173, -0.3085,  ..., -1.3036, -0.4282,  0.1333],\n",
      "         [ 0.0301, -1.9173, -0.3085,  ..., -1.3036, -0.4282,  0.1333],\n",
      "         [ 0.0301, -1.9173, -0.3085,  ..., -1.3036, -0.4282,  0.1333],\n",
      "         ...,\n",
      "         [ 0.0301, -1.9173, -0.3085,  ..., -1.3036, -0.4282,  0.1333],\n",
      "         [ 0.0301, -1.9173, -0.3085,  ..., -1.3036, -0.4282,  0.1333],\n",
      "         [ 0.0301, -1.9173, -0.3085,  ..., -1.3036, -0.4282,  0.1333]],\n",
      "\n",
      "        [[-0.0650, -1.7291, -0.3355,  ..., -1.1321, -0.4478, -0.0151],\n",
      "         [-0.0650, -1.7291, -0.3355,  ..., -1.1321, -0.4478, -0.0151],\n",
      "         [-0.0650, -1.7291, -0.3355,  ..., -1.1321, -0.4478, -0.0151],\n",
      "         ...,\n",
      "         [-0.0650, -1.7291, -0.3355,  ..., -1.1321, -0.4478, -0.0151],\n",
      "         [-0.0650, -1.7291, -0.3355,  ..., -1.1321, -0.4478, -0.0151],\n",
      "         [-0.0650, -1.7291, -0.3355,  ..., -1.1321, -0.4478, -0.0151]],\n",
      "\n",
      "        [[-0.0236, -1.8538, -0.3312,  ..., -1.3128, -0.4112,  0.0698],\n",
      "         [-0.0236, -1.8538, -0.3312,  ..., -1.3128, -0.4112,  0.0698],\n",
      "         [-0.0236, -1.8538, -0.3312,  ..., -1.3128, -0.4112,  0.0698],\n",
      "         ...,\n",
      "         [-0.0236, -1.8538, -0.3312,  ..., -1.3128, -0.4112,  0.0698],\n",
      "         [-0.0236, -1.8538, -0.3312,  ..., -1.3128, -0.4112,  0.0698],\n",
      "         [-0.0236, -1.8538, -0.3312,  ..., -1.3128, -0.4112,  0.0698]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1427, -1.9201, -0.3353,  ..., -1.4637, -0.3647,  0.1768],\n",
      "         [ 0.1427, -1.9201, -0.3353,  ..., -1.4637, -0.3647,  0.1768],\n",
      "         [ 0.1427, -1.9201, -0.3353,  ..., -1.4637, -0.3647,  0.1768],\n",
      "         ...,\n",
      "         [ 0.1427, -1.9201, -0.3353,  ..., -1.4637, -0.3647,  0.1768],\n",
      "         [ 0.1427, -1.9201, -0.3353,  ..., -1.4637, -0.3647,  0.1768],\n",
      "         [ 0.1427, -1.9201, -0.3353,  ..., -1.4637, -0.3647,  0.1768]],\n",
      "\n",
      "        [[ 0.0935, -1.9607, -0.3726,  ..., -1.3687, -0.3394,  0.1409],\n",
      "         [ 0.0935, -1.9607, -0.3726,  ..., -1.3687, -0.3394,  0.1409],\n",
      "         [ 0.0935, -1.9607, -0.3726,  ..., -1.3687, -0.3394,  0.1409],\n",
      "         ...,\n",
      "         [ 0.0935, -1.9607, -0.3726,  ..., -1.3687, -0.3394,  0.1409],\n",
      "         [ 0.0935, -1.9607, -0.3726,  ..., -1.3687, -0.3394,  0.1409],\n",
      "         [ 0.0935, -1.9607, -0.3726,  ..., -1.3687, -0.3394,  0.1409]],\n",
      "\n",
      "        [[ 0.0139, -1.8876, -0.3388,  ..., -1.3349, -0.4185,  0.1173],\n",
      "         [ 0.0139, -1.8876, -0.3388,  ..., -1.3349, -0.4185,  0.1173],\n",
      "         [ 0.0139, -1.8876, -0.3388,  ..., -1.3349, -0.4185,  0.1173],\n",
      "         ...,\n",
      "         [ 0.0139, -1.8876, -0.3388,  ..., -1.3349, -0.4185,  0.1173],\n",
      "         [ 0.0139, -1.8876, -0.3388,  ..., -1.3349, -0.4185,  0.1173],\n",
      "         [ 0.0139, -1.8876, -0.3388,  ..., -1.3349, -0.4185,  0.1173]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0389,  0.3007,  1.2301,  ...,  1.5608,  0.4548, -0.5137],\n",
      "         [-0.0389,  0.3007,  1.2301,  ...,  1.5608,  0.4548, -0.5137],\n",
      "         [-0.0389,  0.3007,  1.2301,  ...,  1.5608,  0.4548, -0.5137],\n",
      "         ...,\n",
      "         [-0.0389,  0.3007,  1.2301,  ...,  1.5608,  0.4548, -0.5137],\n",
      "         [-0.0389,  0.3007,  1.2301,  ...,  1.5608,  0.4548, -0.5137],\n",
      "         [-0.0389,  0.3007,  1.2301,  ...,  1.5608,  0.4548, -0.5137]],\n",
      "\n",
      "        [[-0.2209,  0.4500,  1.1829,  ...,  1.5638,  0.1387, -0.5368],\n",
      "         [-0.2209,  0.4500,  1.1829,  ...,  1.5638,  0.1387, -0.5368],\n",
      "         [-0.2209,  0.4500,  1.1829,  ...,  1.5638,  0.1387, -0.5368],\n",
      "         ...,\n",
      "         [-0.2209,  0.4500,  1.1829,  ...,  1.5638,  0.1387, -0.5368],\n",
      "         [-0.2209,  0.4500,  1.1829,  ...,  1.5638,  0.1387, -0.5368],\n",
      "         [-0.2209,  0.4500,  1.1829,  ...,  1.5638,  0.1387, -0.5368]],\n",
      "\n",
      "        [[-0.0714,  0.3240,  1.1715,  ...,  1.5887,  0.3352, -0.5173],\n",
      "         [-0.0714,  0.3240,  1.1715,  ...,  1.5887,  0.3352, -0.5173],\n",
      "         [-0.0714,  0.3240,  1.1715,  ...,  1.5887,  0.3352, -0.5173],\n",
      "         ...,\n",
      "         [-0.0714,  0.3240,  1.1715,  ...,  1.5887,  0.3352, -0.5173],\n",
      "         [-0.0714,  0.3240,  1.1715,  ...,  1.5887,  0.3352, -0.5173],\n",
      "         [-0.0714,  0.3240,  1.1715,  ...,  1.5887,  0.3352, -0.5173]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3407,  0.0921,  1.0764,  ...,  1.5223,  0.7539, -0.4947],\n",
      "         [ 0.3407,  0.0921,  1.0764,  ...,  1.5223,  0.7539, -0.4947],\n",
      "         [ 0.3407,  0.0921,  1.0764,  ...,  1.5223,  0.7539, -0.4947],\n",
      "         ...,\n",
      "         [ 0.3407,  0.0921,  1.0764,  ...,  1.5223,  0.7539, -0.4947],\n",
      "         [ 0.3407,  0.0921,  1.0764,  ...,  1.5223,  0.7539, -0.4947],\n",
      "         [ 0.3407,  0.0921,  1.0764,  ...,  1.5223,  0.7539, -0.4947]],\n",
      "\n",
      "        [[ 0.4648,  0.1080,  1.0140,  ...,  1.6603,  0.6856, -0.5767],\n",
      "         [ 0.4648,  0.1080,  1.0140,  ...,  1.6603,  0.6856, -0.5767],\n",
      "         [ 0.4648,  0.1080,  1.0140,  ...,  1.6603,  0.6856, -0.5767],\n",
      "         ...,\n",
      "         [ 0.4648,  0.1080,  1.0140,  ...,  1.6603,  0.6856, -0.5767],\n",
      "         [ 0.4648,  0.1080,  1.0140,  ...,  1.6603,  0.6856, -0.5767],\n",
      "         [ 0.4648,  0.1080,  1.0140,  ...,  1.6603,  0.6856, -0.5767]],\n",
      "\n",
      "        [[ 0.0354,  0.2661,  1.1809,  ...,  1.5936,  0.4524, -0.5094],\n",
      "         [ 0.0354,  0.2661,  1.1809,  ...,  1.5936,  0.4524, -0.5094],\n",
      "         [ 0.0354,  0.2661,  1.1809,  ...,  1.5936,  0.4524, -0.5094],\n",
      "         ...,\n",
      "         [ 0.0354,  0.2661,  1.1809,  ...,  1.5936,  0.4524, -0.5094],\n",
      "         [ 0.0354,  0.2661,  1.1809,  ...,  1.5936,  0.4524, -0.5094],\n",
      "         [ 0.0354,  0.2661,  1.1809,  ...,  1.5936,  0.4524, -0.5094]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.0031,  1.0973,  1.1295,  ..., -0.0809,  2.0357, -0.5948],\n",
      "         [-1.0031,  1.0973,  1.1295,  ..., -0.0809,  2.0357, -0.5948],\n",
      "         [-1.0031,  1.0973,  1.1295,  ..., -0.0809,  2.0357, -0.5948],\n",
      "         ...,\n",
      "         [-1.0031,  1.0973,  1.1295,  ..., -0.0809,  2.0357, -0.5948],\n",
      "         [-1.0031,  1.0973,  1.1295,  ..., -0.0809,  2.0357, -0.5948],\n",
      "         [-1.0031,  1.0973,  1.1295,  ..., -0.0809,  2.0357, -0.5948]],\n",
      "\n",
      "        [[-1.2187,  0.8811,  0.8949,  ...,  0.1499,  2.0152, -0.6556],\n",
      "         [-1.2187,  0.8811,  0.8949,  ...,  0.1499,  2.0152, -0.6556],\n",
      "         [-1.2187,  0.8811,  0.8949,  ...,  0.1499,  2.0152, -0.6556],\n",
      "         ...,\n",
      "         [-1.2187,  0.8811,  0.8949,  ...,  0.1499,  2.0152, -0.6556],\n",
      "         [-1.2187,  0.8811,  0.8949,  ...,  0.1499,  2.0152, -0.6556],\n",
      "         [-1.2187,  0.8811,  0.8949,  ...,  0.1499,  2.0152, -0.6556]],\n",
      "\n",
      "        [[-1.1043,  1.0484,  0.9764,  ..., -0.0392,  2.0162, -0.5742],\n",
      "         [-1.1043,  1.0484,  0.9764,  ..., -0.0392,  2.0162, -0.5742],\n",
      "         [-1.1043,  1.0483,  0.9764,  ..., -0.0392,  2.0162, -0.5742],\n",
      "         ...,\n",
      "         [-1.1043,  1.0484,  0.9764,  ..., -0.0392,  2.0162, -0.5742],\n",
      "         [-1.1043,  1.0483,  0.9764,  ..., -0.0392,  2.0162, -0.5742],\n",
      "         [-1.1043,  1.0483,  0.9764,  ..., -0.0392,  2.0162, -0.5742]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8403,  1.1933,  1.2718,  ..., -0.3902,  2.0528, -0.4885],\n",
      "         [-0.8403,  1.1933,  1.2718,  ..., -0.3902,  2.0528, -0.4886],\n",
      "         [-0.8403,  1.1933,  1.2718,  ..., -0.3902,  2.0528, -0.4886],\n",
      "         ...,\n",
      "         [-0.8403,  1.1933,  1.2718,  ..., -0.3902,  2.0528, -0.4885],\n",
      "         [-0.8403,  1.1933,  1.2718,  ..., -0.3902,  2.0528, -0.4885],\n",
      "         [-0.8403,  1.1933,  1.2718,  ..., -0.3902,  2.0528, -0.4885]],\n",
      "\n",
      "        [[-0.8952,  1.0646,  1.2750,  ..., -0.3800,  2.1176, -0.4938],\n",
      "         [-0.8952,  1.0646,  1.2750,  ..., -0.3800,  2.1176, -0.4938],\n",
      "         [-0.8952,  1.0646,  1.2750,  ..., -0.3800,  2.1176, -0.4938],\n",
      "         ...,\n",
      "         [-0.8952,  1.0646,  1.2750,  ..., -0.3800,  2.1176, -0.4938],\n",
      "         [-0.8952,  1.0646,  1.2750,  ..., -0.3800,  2.1176, -0.4938],\n",
      "         [-0.8952,  1.0646,  1.2750,  ..., -0.3800,  2.1176, -0.4938]],\n",
      "\n",
      "        [[-1.0271,  1.0601,  1.0999,  ..., -0.1291,  2.0606, -0.5666],\n",
      "         [-1.0271,  1.0601,  1.0999,  ..., -0.1291,  2.0606, -0.5666],\n",
      "         [-1.0271,  1.0601,  1.0999,  ..., -0.1291,  2.0606, -0.5666],\n",
      "         ...,\n",
      "         [-1.0271,  1.0601,  1.0999,  ..., -0.1291,  2.0606, -0.5666],\n",
      "         [-1.0271,  1.0601,  1.0999,  ..., -0.1291,  2.0606, -0.5666],\n",
      "         [-1.0271,  1.0601,  1.0999,  ..., -0.1291,  2.0606, -0.5666]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)] tensor([[-0.0989,  0.3311,  0.3790,  ...,  0.5544, -0.4642, -0.3981],\n",
      "        [-0.3253,  0.3708,  0.4648,  ...,  0.4786, -0.4454, -0.3113],\n",
      "        [-0.1535,  0.3206,  0.3977,  ...,  0.5325, -0.4001, -0.3405],\n",
      "        ...,\n",
      "        [ 0.1400,  0.2193,  0.2381,  ...,  0.6091, -0.4198, -0.4491],\n",
      "        [ 0.0443,  0.2120,  0.2232,  ...,  0.5521, -0.4134, -0.4148],\n",
      "        [-0.0922,  0.3004,  0.3641,  ...,  0.5601, -0.4308, -0.3763]],\n",
      "       grad_fn=<TanhBackward0>) tensor([[-0.1676,  1.3153,  1.9797,  ...,  0.1362,  1.1185,  1.1715],\n",
      "        [-0.1507,  1.3954,  2.1379,  ...,  0.0619,  1.1596,  1.2607],\n",
      "        [-0.1583,  1.2890,  2.0030,  ...,  0.1131,  1.0808,  1.2455],\n",
      "        ...,\n",
      "        [-0.1696,  1.0244,  1.7425,  ...,  0.2241,  0.9144,  1.1687],\n",
      "        [-0.1674,  1.2263,  1.9197,  ...,  0.1849,  0.9402,  1.1537],\n",
      "        [-0.1666,  1.2711,  1.9801,  ...,  0.1305,  1.0650,  1.2283]],\n",
      "       grad_fn=<GeluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder_output, pooled_output, output = model(**data)\n",
    "\n",
    "print(encoder_output, pooled_output, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visual_embed(pixel_values, pixel_mask, max_image_length=200):\n",
    "    patch_embeddings = PatchEmbeddings(config)\n",
    "    dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    print(pixel_values.size())\n",
    "    cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "    num_patches = patch_embeddings.num_patches\n",
    "    position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
    "    _, _, ph, pw = patch_embeddings.projection.weight.shape\n",
    "\n",
    "    x = patch_embeddings(pixel_values)\n",
    "    print(x.size())\n",
    "    x_mask = pixel_mask[:, None, :, :].float()\n",
    "    x_mask = nn.functional.interpolate(x_mask, size=(x.shape[2], x.shape[3])).long()\n",
    "    x_h = x_mask[:, 0].sum(dim=1)[:, 0]\n",
    "    x_w = x_mask[:, 0].sum(dim=2)[:, 0]\n",
    "\n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    patch_dim = config.image_size // config.patch_size\n",
    "    spatial_pos = position_embeddings[:, 1:, :].transpose(1, 2).view(1, num_channels, patch_dim, patch_dim)\n",
    "    pos_embed = torch.cat(\n",
    "        [\n",
    "            nn.functional.pad(\n",
    "                nn.functional.interpolate(\n",
    "                    spatial_pos,\n",
    "                    size=(h, w),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=True,\n",
    "                ),\n",
    "                (0, width - w, 0, height - h),\n",
    "            )\n",
    "            for h, w in zip(x_h, x_w)\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    pos_embed = pos_embed.flatten(2).transpose(1, 2)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "        # Set `device` here, otherwise `patch_index` will always be on `CPU` and will fail near the end for torch>=1.13\n",
    "    patch_index = torch.stack(\n",
    "        torch.meshgrid(torch.arange(x_mask.shape[-2]), torch.arange(x_mask.shape[-1]), indexing=\"ij\"), dim=-1\n",
    "    ).to(device=x_mask.device)\n",
    "    patch_index = patch_index[None, None, :, :, :]\n",
    "    patch_index = patch_index.expand(x_mask.shape[0], x_mask.shape[1], -1, -1, -1)\n",
    "    patch_index = patch_index.flatten(1, 3)\n",
    "    x_mask = x_mask.flatten(1)\n",
    "\n",
    "    if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n",
    "        # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n",
    "        # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n",
    "        # if self.patch_size = 32, 25 * 41 = 1025\n",
    "        # if res is 384 x 640, 12 * 20 = 240\n",
    "        effective_resolution = x_h * x_w\n",
    "        max_image_length = effective_resolution.max()\n",
    "    else:\n",
    "        effective_resolution = x_h * x_w\n",
    "        max_image_length = min(effective_resolution.max(), max_image_length)\n",
    "\n",
    "    valid_idx = x_mask.nonzero(as_tuple=False)\n",
    "    non_valid_idx = (1 - x_mask).nonzero(as_tuple=False)\n",
    "    unique_rows = valid_idx[:, 0].unique()\n",
    "    valid_row_idx = [valid_idx[valid_idx[:, 0] == u] for u in unique_rows]\n",
    "    non_valid_row_idx = [non_valid_idx[non_valid_idx[:, 0] == u] for u in unique_rows]\n",
    "    valid_nums = [v.size(0) for v in valid_row_idx]\n",
    "    non_valid_nums = [v.size(0) for v in non_valid_row_idx]\n",
    "    pad_nums = [max_image_length - v for v in valid_nums]\n",
    "\n",
    "    select = list()\n",
    "    for i, (v, nv, p) in enumerate(zip(valid_nums, non_valid_nums, pad_nums)):\n",
    "        if p <= 0:\n",
    "            valid_choice = torch.multinomial(torch.ones(v).float(), max_image_length)\n",
    "            select.append(valid_row_idx[i][valid_choice])\n",
    "        else:\n",
    "            pad_choice = torch.multinomial(torch.ones(nv).float(), p, replacement=True)\n",
    "            select.append(torch.cat([valid_row_idx[i], non_valid_row_idx[i][pad_choice]], dim=0))\n",
    "\n",
    "    select = torch.cat(select, dim=0)\n",
    "    x = x[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "    x_mask = x_mask[select[:, 0], select[:, 1]].view(batch_size, -1)\n",
    "    # `patch_index` should be on the same device as `select` (for torch>=1.13), which is ensured at definition time.\n",
    "    patch_index = patch_index[select[:, 0], select[:, 1]].view(batch_size, -1, 2)\n",
    "    pos_embed = pos_embed[select[:, 0], select[:, 1]].view(batch_size, -1, num_channels)\n",
    "\n",
    "    cls_tokens = cls_token.expand(batch_size, -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "    pos_embed = torch.cat(\n",
    "        (position_embeddings[:, 0, :][:, None, :].expand(batch_size, -1, -1), pos_embed), dim=1\n",
    "    )\n",
    "    x = x + pos_embed\n",
    "    x = dropout(x)\n",
    "\n",
    "    x_mask = torch.cat([torch.ones(x_mask.shape[0], 1).to(x_mask), x_mask], dim=1)\n",
    "\n",
    "    return x, x_mask, (patch_index, (height, width))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 576, 608])\n",
      "torch.Size([32, 768, 18, 19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.1356e-01, -2.4815e-01,  1.4488e-01,  ...,  1.2924e-02,\n",
       "           -1.2265e-01, -3.1479e-02],\n",
       "          [ 1.2028e-01, -2.2767e-01,  1.6437e-01,  ...,  4.6188e-03,\n",
       "           -8.7430e-02, -1.3199e-02],\n",
       "          ...,\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.2459e-01,  6.4102e-01,  8.0885e-02,  ...,  2.9746e-02,\n",
       "            9.6536e-01,  5.3113e-01],\n",
       "          [ 2.4830e-01,  6.1691e-01,  7.4150e-02,  ...,  7.7011e-02,\n",
       "            9.7985e-01,  5.2313e-01],\n",
       "          ...,\n",
       "          [ 6.2098e-02,  4.5380e-01,  2.7075e-02,  ...,  4.4142e-02,\n",
       "            1.0477e+00,  5.3423e-01],\n",
       "          [-6.1311e-02, -7.2733e-01, -3.6356e-04,  ...,  3.6458e-02,\n",
       "           -7.9055e-01, -4.6743e-01],\n",
       "          [-1.0062e-01, -1.0580e+00, -1.1916e+00,  ...,  3.8187e-02,\n",
       "           -3.8073e-01, -1.5508e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.4028e-01,  3.3301e-01, -5.4992e-02,  ..., -2.3290e-02,\n",
       "            6.3017e-01,  3.7428e-01],\n",
       "          [ 1.4207e-01,  3.4607e-01, -4.6306e-02,  ..., -1.3059e-03,\n",
       "            6.2293e-01,  3.4566e-01],\n",
       "          ...,\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.0298e-01, -6.6000e-01, -8.0807e-02,  ..., -1.7845e-02,\n",
       "           -9.4984e-01, -5.1209e-01],\n",
       "          [-2.0284e-01, -6.6008e-01, -8.0877e-02,  ..., -1.7737e-02,\n",
       "           -9.5017e-01, -5.1194e-01],\n",
       "          ...,\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.0083e-02,  2.4418e-01,  1.0291e-01,  ...,  5.5176e-03,\n",
       "           -1.8328e-01,  5.8304e-02],\n",
       "          [ 2.1224e-01,  4.7702e-01,  2.1632e-01,  ...,  1.3448e-01,\n",
       "            5.9655e-01,  1.5304e-01],\n",
       "          ...,\n",
       "          [-2.6530e-01, -1.4323e-01,  2.0794e-01,  ...,  5.5848e-02,\n",
       "           -3.3645e-01, -6.8175e-03],\n",
       "          [-1.2329e-01, -3.3308e-01, -6.8487e-02,  ..., -1.5400e-02,\n",
       "           -3.5407e-01, -2.3716e-01],\n",
       "          [-9.9644e-02, -3.5794e-01,  7.6390e-02,  ...,  1.3617e-01,\n",
       "           -4.0707e-01, -1.5922e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-1.6834e-01, -1.6462e-01, -3.8337e-01,  ..., -6.7978e-02,\n",
       "            2.7167e-04, -1.6488e-01],\n",
       "          [-4.7313e-03, -6.4985e-01, -2.4830e-01,  ...,  6.9465e-02,\n",
       "           -4.7218e-01, -1.3625e-01],\n",
       "          ...,\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03],\n",
       "          [ 1.1348e-02, -1.1186e-02,  1.8752e-03,  ...,  7.7177e-03,\n",
       "            4.7898e-03,  9.3082e-03]]], grad_fn=<AddBackward0>),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " (tensor([[[ 0,  0],\n",
       "           [ 0,  1],\n",
       "           [ 0,  2],\n",
       "           ...,\n",
       "           [17,  1],\n",
       "           [14, 18],\n",
       "           [10, 18]],\n",
       "  \n",
       "          [[10, 15],\n",
       "           [ 5, 12],\n",
       "           [ 4, 10],\n",
       "           ...,\n",
       "           [10,  1],\n",
       "           [11, 10],\n",
       "           [ 8,  9]],\n",
       "  \n",
       "          [[ 0,  0],\n",
       "           [ 0,  1],\n",
       "           [ 0,  2],\n",
       "           ...,\n",
       "           [13, 17],\n",
       "           [17, 17],\n",
       "           [14, 16]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 0,  0],\n",
       "           [ 0,  1],\n",
       "           [ 0,  2],\n",
       "           ...,\n",
       "           [ 1, 16],\n",
       "           [ 0, 18],\n",
       "           [ 4, 16]],\n",
       "  \n",
       "          [[ 7,  1],\n",
       "           [ 9, 12],\n",
       "           [ 7,  0],\n",
       "           ...,\n",
       "           [ 3,  7],\n",
       "           [ 8,  1],\n",
       "           [ 0, 16]],\n",
       "  \n",
       "          [[ 0,  0],\n",
       "           [ 0,  1],\n",
       "           [ 0,  2],\n",
       "           ...,\n",
       "           [13,  3],\n",
       "           [ 5, 16],\n",
       "           [ 6, 15]]]),\n",
       "  (18, 19)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vilt_model import ViltEmbeddings\n",
    "s= data\n",
    "\n",
    "visual_embed(s[\"pixel_values\"],s[\"pixel_mask\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPVisionModel: ['visual_projection.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_projection.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPVisionModel\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())\n",
    "# last_hidden_state = outputs.last_hidden_state\n",
    "# pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa38bdac91f012d3aa63752192202c1c4a31351885bf71923469150acc0ed662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
